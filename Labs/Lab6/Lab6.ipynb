{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e723e203",
   "metadata": {},
   "source": [
    "# Lab 6: Student Performance Prediction with Tree-Based Models and Naive Bayes\n",
    "\n",
    "## Overview\n",
    "This lab explores **supervised learning classification** using three powerful algorithms:\n",
    "1. **Decision Trees** - Interpretable tree-structured models\n",
    "2. **Random Forests** - Ensemble of decision trees\n",
    "3. **Naive Bayes** - Probabilistic classifier based on Bayes' theorem\n",
    "\n",
    "**Dataset**: Student Performance Dataset\n",
    "- Academic performance of Portuguese secondary school students\n",
    "- Includes demographic, social, and academic features\n",
    "- **Goal**: Predict whether a student will pass (final grade ≥ 10)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will understand:\n",
    "- How **Decision Trees** partition data using feature splits\n",
    "- How **Random Forests** reduce overfitting through ensemble learning\n",
    "- How **Naive Bayes** uses probability theory for classification\n",
    "- The importance of **binary classification** and threshold selection\n",
    "- How to compare models using **accuracy** and **confusion matrices**\n",
    "- The trade-offs between model complexity, interpretability, and performance\n",
    "\n",
    "## Why These Algorithms?\n",
    "\n",
    "**Decision Trees**:\n",
    "- ✅ Highly interpretable (can visualize decision rules)\n",
    "- ✅ Handle non-linear relationships\n",
    "- ❌ Prone to overfitting\n",
    "- ❌ High variance (small data changes → big tree changes)\n",
    "\n",
    "**Random Forests**:\n",
    "- ✅ Reduces overfitting through averaging\n",
    "- ✅ Often best \"out-of-the-box\" performance\n",
    "- ❌ Less interpretable than single trees\n",
    "- ❌ More computationally expensive\n",
    "\n",
    "**Naive Bayes**:\n",
    "- ✅ Very fast training and prediction\n",
    "- ✅ Works well with small datasets\n",
    "- ✅ Good with high-dimensional data\n",
    "- ❌ Assumes feature independence (often violated in practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544017c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import modules for machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afb5d0",
   "metadata": {},
   "source": [
    "## 1. Library Imports\n",
    "\n",
    "### Data Manipulation Libraries\n",
    "- **pandas**: For loading and manipulating tabular data\n",
    "- **numpy**: For numerical operations and array handling\n",
    "- **matplotlib/seaborn**: For data visualization\n",
    "\n",
    "### Scikit-learn Modules\n",
    "\n",
    "**Model Selection:**\n",
    "- `train_test_split`: Splits data into training and testing sets\n",
    "- `GridSearchCV`: Automated hyperparameter tuning with cross-validation\n",
    "\n",
    "**Classifiers:**\n",
    "- `DecisionTreeClassifier`: Single decision tree model\n",
    "- `RandomForestClassifier`: Ensemble of decision trees (bagging)\n",
    "- `GaussianNB`: Naive Bayes assuming features follow Gaussian (normal) distribution\n",
    "- `MultinomialNB`: Naive Bayes for discrete count features\n",
    "- `BernoulliNB`: Naive Bayes for binary/Boolean features\n",
    "\n",
    "**Preprocessing:**\n",
    "- `LabelEncoder`: Convert categorical labels to integers\n",
    "- `StandardScaler`: Standardize features (mean=0, std=1)\n",
    "- `OneHotEncoder`: Convert categorical features to binary vectors\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- `confusion_matrix`: Show true/false positives and negatives\n",
    "- `accuracy_score`: Percentage of correct predictions\n",
    "- `classification_report`: Precision, recall, F1-score for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83df3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "school",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sex",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "address",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "famsize",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Pstatus",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Medu",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Fedu",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Mjob",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Fjob",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reason",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "guardian",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "traveltime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "studytime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "failures",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "schoolsup",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "famsup",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "paid",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "activities",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nursery",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "higher",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "internet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "romantic",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "famrel",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "freetime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "goout",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dalc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Walc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "health",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "absences",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "G1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "G2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "G3",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ee141ca6-2d78-4f93-b457-db9a1a09e8cf",
       "rows": [
        [
         "0",
         "GP",
         "F",
         "18",
         "U",
         "GT3",
         "A",
         "4",
         "4",
         "at_home",
         "teacher",
         "course",
         "mother",
         "2",
         "2",
         "0",
         "yes",
         "no",
         "no",
         "no",
         "yes",
         "yes",
         "no",
         "no",
         "4",
         "3",
         "4",
         "1",
         "1",
         "3",
         "4",
         "0",
         "11",
         "11"
        ],
        [
         "1",
         "GP",
         "F",
         "17",
         "U",
         "GT3",
         "T",
         "1",
         "1",
         "at_home",
         "other",
         "course",
         "father",
         "1",
         "2",
         "0",
         "no",
         "yes",
         "no",
         "no",
         "no",
         "yes",
         "yes",
         "no",
         "5",
         "3",
         "3",
         "1",
         "1",
         "3",
         "2",
         "9",
         "11",
         "11"
        ],
        [
         "2",
         "GP",
         "F",
         "15",
         "U",
         "LE3",
         "T",
         "1",
         "1",
         "at_home",
         "other",
         "other",
         "mother",
         "1",
         "2",
         "0",
         "yes",
         "no",
         "no",
         "no",
         "yes",
         "yes",
         "yes",
         "no",
         "4",
         "3",
         "2",
         "2",
         "3",
         "3",
         "6",
         "12",
         "13",
         "12"
        ],
        [
         "3",
         "GP",
         "F",
         "15",
         "U",
         "GT3",
         "T",
         "4",
         "2",
         "health",
         "services",
         "home",
         "mother",
         "1",
         "3",
         "0",
         "no",
         "yes",
         "no",
         "yes",
         "yes",
         "yes",
         "yes",
         "yes",
         "3",
         "2",
         "2",
         "1",
         "1",
         "5",
         "0",
         "14",
         "14",
         "14"
        ]
       ],
       "shape": {
        "columns": 33,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  ...  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher  ...   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other  ...   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other  ...   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services  ...   \n",
       "\n",
       "  famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
       "0      4        3      4     1     1      3        4   0  11  11  \n",
       "1      5        3      3     1     1      3        2   9  11  11  \n",
       "2      4        3      2     2     3      3        6  12  13  12  \n",
       "3      3        2      2     1     1      5        0  14  14  14  \n",
       "\n",
       "[4 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/student_performance.csv')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088253f",
   "metadata": {},
   "source": [
    "## 2. Loading the Student Performance Dataset\n",
    "\n",
    "### About the Dataset\n",
    "This dataset contains information about student achievement in Portuguese secondary education.\n",
    "\n",
    "**Key Features Include:**\n",
    "- **Demographics**: Age, sex, family size, parent education\n",
    "- **Social factors**: Family relationships, free time, going out, alcohol consumption\n",
    "- **Academic factors**: Study time, past failures, absences\n",
    "- **Grades**: G1 (period 1), G2 (period 2), G3 (final grade)\n",
    "\n",
    "### Functions Used:\n",
    "\n",
    "**`pd.read_csv()`**:\n",
    "- Reads CSV file into a pandas DataFrame\n",
    "- Automatically infers data types for each column\n",
    "\n",
    "**`df.head(n)`**:\n",
    "- Returns first n rows of the DataFrame\n",
    "- Quick preview to understand data structure\n",
    "\n",
    "**`df.info()`**:\n",
    "- Shows column names, data types, and non-null counts\n",
    "- Helps identify missing values and data type issues\n",
    "- Displays memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494994f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 649 entries, 0 to 648\n",
      "Data columns (total 33 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   school      649 non-null    object\n",
      " 1   sex         649 non-null    object\n",
      " 2   age         649 non-null    int64 \n",
      " 3   address     649 non-null    object\n",
      " 4   famsize     649 non-null    object\n",
      " 5   Pstatus     649 non-null    object\n",
      " 6   Medu        649 non-null    int64 \n",
      " 7   Fedu        649 non-null    int64 \n",
      " 8   Mjob        649 non-null    object\n",
      " 9   Fjob        649 non-null    object\n",
      " 10  reason      649 non-null    object\n",
      " 11  guardian    649 non-null    object\n",
      " 12  traveltime  649 non-null    int64 \n",
      " 13  studytime   649 non-null    int64 \n",
      " 14  failures    649 non-null    int64 \n",
      " 15  schoolsup   649 non-null    object\n",
      " 16  famsup      649 non-null    object\n",
      " 17  paid        649 non-null    object\n",
      " 18  activities  649 non-null    object\n",
      " 19  nursery     649 non-null    object\n",
      " 20  higher      649 non-null    object\n",
      " 21  internet    649 non-null    object\n",
      " 22  romantic    649 non-null    object\n",
      " 23  famrel      649 non-null    int64 \n",
      " 24  freetime    649 non-null    int64 \n",
      " 25  goout       649 non-null    int64 \n",
      " 26  Dalc        649 non-null    int64 \n",
      " 27  Walc        649 non-null    int64 \n",
      " 28  health      649 non-null    int64 \n",
      " 29  absences    649 non-null    int64 \n",
      " 30  G1          649 non-null    int64 \n",
      " 31  G2          649 non-null    int64 \n",
      " 32  G3          649 non-null    int64 \n",
      "dtypes: int64(16), object(17)\n",
      "memory usage: 167.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0b3fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAANuZJREFUeJzt3QuYTXX///+38/ksg3IsOUVKEZ3uIupWke6OhJRKOuBOpah0kuSQc9wRRcr3js6E5K4oNUrpQEoSoYMxUUaxftfr8//vfe0ZM4Mxa/b+jOfjurax1957fdZee++1XutzWKtAEASBAQAAeKhgvBcAAAAgpwgyAADAWwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDJACHr06GG1a9eOSzkFChSwBx54IPSy33nnHVeW/kb84x//sBNOOMHywvfff+/Kf+aZZywenn32WWvQoIEVKVLEypcvH5dlAECQQT6zfv16u+WWW+z444+3kiVLulujRo2sT58+9tlnn8V78RLWrFmzbPTo0ZaIEnHZvv76axcijz32WJsyZYpNnjw53ouU76xZs8b69etnrVu3tuLFi7vQqvAKZFR4vymAp1577TW74oorrHDhwtalSxc78cQTrWDBgm6n89JLL9nEiRNd0KlVq5blZ3/++adbB4caFlavXm19+/Y96NecddZZrqyiRYvmYCkPf9n0Oap81YjkNdVC7du3z5588kk77rjj8rz8I8Hy5cttzJgx7kCkYcOG9umnn8Z7kZCgCDLIF7799lu78sor3c5t8eLFVq1atXSPDxs2zCZMmOCCTXZ27dplpUqVMp/p6DVMu3fvduFF6zLssrKjI/R4lb9t2zb3NzeblP744w9Xg5jIdI1hff4lSpQIvayLL77YUlJSrEyZMvbEE08QZJAlmpaQLzz++OMuhEybNm2/ECOqobjtttusRo0a0WlqGihdurQLQf/85z/dBlM1OfLuu+/aZZddZjVr1rRixYq516maWzUAGc2bN8/1C9FOVX/nzp2b6TLqCF5NJI0bN3bPTUpKshtvvNG2b99+UO/xYMvJ2Efm999/d7UZ6kuj91KlShU777zzbOXKldF+La+//rpt2LDBvVa3SL+bSD+Y2bNn26BBg+zoo492O9vU1NRM+8hEJCcnuyYB7fDq1KljkyZNSve4+rVk1lSQcZ7ZLVtWfWTefvttO/PMM10gVdDo2LGjffXVV+meo/Wj165bt859D/S8cuXK2bXXXusCRXZU/v333+/+f9RRR+23vhWY9RlrXVevXt01a2qHHCvSl0jrSTVbWqf33HNPtuWqZvHyyy93ZWq91q9f3+699950z/nkk0/sggsusLJly7rvdps2beyDDz7I9L1nlNlnovd64YUX2oIFC+yUU05x5T711FPusYULF9oZZ5zh1p3K0vJkfA9paWluXanWKvI7uvPOO930A6lYsaL7TQIHQo0M8k2zkjaWLVu2PKTX/f3339a+fXu3QdZRX+SIeM6cOW6H1rt3b6tUqZKtWLHCxo4daz/++KN7LOKtt96ySy+91FV/Dx061H799Ve3MzzmmGP2K0uhRTsLPa5QpWaucePGuZ3P+++/n20TyaGUk9FNN91k//d//+f6Dun1eu17773ndu4nn3yy2xnu2LHDvbdRo0a512jHFOuhhx5ytTB33HGH2wll15ykYKZgqJ3uVVddZS+++KJbj3pNz5497VAczLLFWrRokduR161b1+2wFTz1uZ1++ukuuGXsGK1lVNDSOtXj//nPf1zQUw1eVhRGZ8yY4YKkmiu1PE2bNnWPqcwhQ4ZY27Zt3XtWPw8956OPPtrvM9bnoGVVTWLXrl1dsM2K+ncpnOn1N9xwg3sfCuCvvvqqPfLII+45X3zxhXuOQozCgp6r0KHQtHTp0kP+bUToPehz1Pe3V69eLrCoLAUcve8HH3zQhRSFQr3H2OCuWhV917TMah76/PPP3ee4du1aF8yBXBEAntuxY0egr3KnTp32e2z79u3Bzz//HL398ccf0ce6d+/uXnf33Xfv97rY50UMHTo0KFCgQLBhw4botGbNmgXVqlULUlJSotPeeustN99atWpFp7377rtu2syZM9PNc/78+ZlOz+hgyxFNu//++6P3y5UrF/Tp0yfb+Xfo0GG/+ciSJUvc/OrWrbvfOok8pr8RZ599tps2YsSI6LS0tDS3/FWqVAn27Nnjpk2bNs09b/369QecZ1bLptfquZpX7HpSOb/++mt02qpVq4KCBQsG3bp1i07T+tFre/bsmW6el1xySVCpUqVs11Xs6/Wditi2bVtQtGjRoF27dsHevXuj08eNG+eeO3Xq1P3W06RJk4KDcdZZZwVlypRJ992Tffv2Rf+v77/K//bbb6PTNm/e7F6n12dc9owy+0y03jVN39NYo0aN2u/9Z/Tss8+69a7vfiy9Z732/fffDw7W8OHDM/2+AELTErynZo6sjtR1NKqq+Mht/Pjx+z1HR84ZxfYBUJPVL7/84ppKlBNUgyI//fSTa7fv3r27a5aIULONaj5iqRZHz9Fjmlfk1rx5c7fcS5YsyfL9HUo5mVHV/4cffmibN2+2nFLZB9svQs14OnqPUE2M7qtfiZpSwhJZT2oqUrNEhGoNtK7eeOONTGurYqlGQzUlke/UoVBt0J49e1wzXmxfLNViqJZETWSxVIuhWrUD+fnnn+1///ufq81SU2esSBPR3r17Xa1dp06dXG1UhJpZr776alcrkpP3JKqxUq1lrEjfoJdfftnVvGRG33nVwmiIeux3/txzz3WPZ/edBw4FQQbei7Sj79y5c7/HVLWutvznnnsuy51uZs0zP/zwQ3SHqKChEHT22We7x9TUIeq3IfXq1dvv9ap+j/XNN9+416nZIjZY6abljnQezcyhlJNV/yGN+lH/hBYtWrjmj++++84OdWd2sNQvJGOHaQ2HlzCHz0bWU2brRDtU7UQVSmNlDAYVKlRwfw+239LBlK8gp3AReTxC/Y0OZsRX5LPK7vw8CjtqCs3qvStsbNy40XIis89eowPVXHf99de7JjE1j6kJMTbU6DuvJqiM3/fIdyG77zxwKOgjA++plkJHntpZZxTpF5DVDlRHxRlHMunoVkfwv/32m911113uiFI75k2bNrlwk9URaHb0GoWYmTNnZvq4NvBhUT8Q1TSoT4eO2ocPH+76gGhIuvpoHIzcHqWSWWfTyLrPS4UKFcp0+v/XQheuvBj5kxvrPrPl1DTVEqlWRTVN8+fPtxdeeMHVtug7pvWq73yTJk1s5MiRmc43tuM9cDgIMsgXOnTo4DpqqlOuah0OhzokqjPi9OnTrVu3btHpqtmJFTkfjY48M+sgGUsnTlPTg45iD3UHdijlZEVB7+abb3Y3HQmrk686iUaCTFY7t5xQE1bGYexanxLpbBup+cg4midjrcWhLFtkPWW2TjTip3LlyqEOrY8tP7Z5R81N6titDsA5EZlXZkE9Ngiro3pW711hPRIcYtd97PDxzNZ9djRPjYrSTWHl0UcfdZ2zFW70XvWdX7VqlXs8N79fQEY0LSFf0CgNbcjVj2Dr1q2HdYQdOUqPfY3+r5OfZQwHzZo1c4En0twUCTxffvnlfrUiOuLV6J/MRk5l3KHntJyMVGbsa0Q1Q2r+iR0Cqx18xufllN5PZIhuZEeu+9rZqk+QaCcnOqqPXdbMzpB7sMsWu55i16cCgGoJNJIqTNp5q6lIJ3GL/e48/fTTbvkVtnNC601DtKdOneqaPGNFytF3tl27dq7PSmzto34LOqGgRuWpn05W617BU+vtYKm2MiOte4l8r/SdVy2mznyckUaTZWzmA3KKGhnkC+o/og22homqn0DkzL7a0OtoWI/pCPJghiurKUkbew011oZYO4D//ve/mfab0LBd7aC0o1CI0gZew311HpHYPjvqX6MOr3q+OqRqp6PhsaplUadIhaR//etfWS7TwZaTkc4ho/eseWt9qL+PaoY0HHjEiBHR5ylgqGmgf//+duqpp7rnXXTRRZYTCklqutIOVf0hNF+9Z4WUyPBjLfdpp51mAwcOdO9FfZF0rhqFoIwOZdnUbKZaplatWtl1110XHX6t5sewrz+lwKH3o+HX559/vht6rBoSnVdGy60h1jmlcKTPXjVpGsqsfitav2rWiZwo7uGHH46e20U1b+r/pQCpYKF+UhH67qlvkNbPgAEDXAhSSNLyZwxKWdGQawUhfSdVE6VaPr1PfddUvlxzzTWu34w6VKuWRrWRCquqIdL0yLlpsqLwp89OIsO6dboC1SLpptMJAA6Dt5CfrFu3Lujdu3dw3HHHBcWLFw9KlCgRNGjQILjpppuCTz/9NN1zNfy6VKlSmc7nyy+/DNq2bRuULl06qFy5ctCrVy83jDfjcF/573//GzRs2DAoVqxY0KhRo+Cll15y885syPDkyZOD5s2bu+XSsNgmTZoEd955pxsmeyAHW07s8GsNfR4wYEBw4oknuvL0fvX/CRMmpHvNzp07g6uvvjooX758uiHdkeHQc+bM2W95shp+3bhx4+Djjz8OWrVq5T4DzUtDkDPSMGGtY72fpKSk4J577gkWLly43zyzWrbMhl/LokWLgtNPP92t47JlywYXXXSR+zwPNHw6u2HhGWX1etF71XeuSJEi7n3p+6jTAMSKrKdDsXr1ajc8XOtB67V+/frB4MGD0z1n5cqVQfv27d33tmTJksE555wTLFu2bL95JScnBy1btnTDtWvWrBmMHDkyy+HXGv6e0eLFi4OOHTsG1atXd/PQ36uuuipYu3ZtuudpuP2wYcPce9XnXKFCBff9HzJkiDttQnYin29mt8x+WzhyFdA/ZDoAAOAj+sgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWQQYAAHgr358QT9f70CnTdWFBTpMNAIAfdHYYndRTJ9nMeE28IyrIKMRwcTIAAPykK7dnd1b2fB9kVBMTWRGRa40AAIDElpqa6ioiIvvxIzbIRJqTFGIIMgAA+OVA3ULo7AsAALxFkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAeIsgAwAAvEWQAQAA3iLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwVuF4LwCAxNV8wIzQ5p08vFto8wZw5KBGBgAAeIsgAwAAvEWQAQAA3iLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbheO9AAAQq/mAGaHOP3l4t1DnDyBvUSMDAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWQQYAAHiLIAMAALxFkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAeIsgAwAAvEWQAQAA3iLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWQQYAAHircLwXAADirfmAGaHOP3l4t1DnDxzJqJEBAADeIsgAAABvxTXI7N271wYPHmx16tSxEiVK2LHHHmsPPfSQBUEQfY7+f99991m1atXcc9q2bWvffPNNPBcbAAAkiLgGmWHDhtnEiRNt3Lhx9tVXX7n7jz/+uI0dOzb6HN0fM2aMTZo0yT788EMrVaqUtW/f3nbv3h3PRQcAAEd6Z99ly5ZZx44drUOHDu5+7dq17fnnn7cVK1ZEa2NGjx5tgwYNcs+TGTNmWFJSks2bN8+uvPLKeC4+AAA4kmtkWrdubYsXL7a1a9e6+6tWrbL33nvPLrjgAnd//fr1tmXLFtecFFGuXDlr2bKlLV++PNN5pqWlWWpqarobAADIn+JaI3P33Xe7oNGgQQMrVKiQ6zPzyCOPWJcuXdzjCjGiGphYuh95LKOhQ4fakCFD8mDpAQDAEV0j8+KLL9rMmTNt1qxZtnLlSps+fbo98cQT7m9ODRw40Hbs2BG9bdy4MVeXGQAAJI641sgMGDDA1cpE+ro0adLENmzY4GpVunfvblWrVnXTt27d6kYtReh+s2bNMp1nsWLF3A0AAOR/ca2R+eOPP6xgwfSLoCamffv2uf9rWLbCjPrRRKgpSqOXWrVqlefLCwAAEktca2Quuugi1yemZs2a1rhxY/vkk09s5MiR1rNnT/d4gQIFrG/fvvbwww9bvXr1XLDReWeqV69unTp1iueiAwCAIz3I6HwxCiY333yzbdu2zQWUG2+80Z0AL+LOO++0Xbt22Q033GApKSl2xhln2Pz586148eLxXHQAAHCkB5kyZcq488TolhXVyjz44IPuBgAAEItrLQEAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAeIsgAwAAvEWQAQAA3iLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWQQYAAHiLIAMAALxVON4LAABHquYDZoQ27+Th3UKbN5BIqJEBAADeIsgAAABvEWQAAIC36CMDAEcQ+uUgv6FGBgAAeIsgAwAAvEWQAQAA3iLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWQQYAAHiLIAMAALxFkAEAAN4iyAAAAG8RZAAAgLcKx3sBAByc5gNmhDbv5OHdQps3AISJGhkAAOAtggwAAPAWQQYAAHiLIAMAALxFkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAeIsgAwAAvEWQAQAA3op7kNm0aZN17drVKlWqZCVKlLAmTZrYxx9/HH08CAK77777rFq1au7xtm3b2jfffBPXZQYAAIkhrkFm+/btdvrpp1uRIkXszTfftC+//NJGjBhhFSpUiD7n8ccftzFjxtikSZPsww8/tFKlSln79u1t9+7d8Vx0AACQAArHs/Bhw4ZZjRo1bNq0adFpderUSVcbM3r0aBs0aJB17NjRTZsxY4YlJSXZvHnz7Morr4zLcgMAgMQQ1xqZV155xU455RS77LLLrEqVKnbSSSfZlClToo+vX7/etmzZ4pqTIsqVK2ctW7a05cuXZzrPtLQ0S01NTXcDAAD5U1yDzHfffWcTJ060evXq2YIFC6x3795222232fTp093jCjGiGphYuh95LKOhQ4e6sBO5qcYHAADkT3ENMvv27bOTTz7ZHn30UVcbc8MNN1ivXr1cf5icGjhwoO3YsSN627hxY64uMwAASBxxDTIaidSoUaN00xo2bGg//PCD+3/VqlXd361bt6Z7ju5HHsuoWLFiVrZs2XQ3AACQP8U1yGjE0po1a9JNW7t2rdWqVSva8VeBZfHixdHH1edFo5datWqV58sLAAASS1xHLfXr189at27tmpYuv/xyW7FihU2ePNndpECBAta3b197+OGHXT8aBZvBgwdb9erVrVOnTvFcdAAAcKQHmVNPPdXmzp3r+rU8+OCDLqhouHWXLl2iz7nzzjtt165drv9MSkqKnXHGGTZ//nwrXrx4PBcdAAAc6UFGLrzwQnfLimplFHJ0AwAASKhLFAAAAOQUQQYAAHiLIAMAALxFkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAeIsgAwAAvEWQAQAA3iLIAACAIyvI1K1b13799df9pqekpLjHAAAAEjbIfP/997Z37979pqelpdmmTZtyY7kAAAAOqLAdgldeeSX6/wULFli5cuWi9xVsFi9ebLVr1z6UWQIAAORNkOnUqZP7W6BAAevevXu6x4oUKeJCzIgRI3K+NAAAAGEFmX379rm/derUsY8++sgqV658KC8HAACIX5CJWL9+fe4vCQAAQF4EGVF/GN22bdsWramJmDp1ak5nCwAAEG6QGTJkiD344IN2yimnWLVq1VyfGQAAAC+CzKRJk+yZZ56xa665JveXCAAAIMzzyOzZs8dat26dk5cCAADEN8hcf/31NmvWrNxbCgAAgLxqWtq9e7dNnjzZFi1aZE2bNnXnkIk1cuTInMwWAAAg/CDz2WefWbNmzdz/V69ene4xOv4CAICEDjJLlizJ/SUBAADIiz4yAAAA3tbInHPOOdk2Ib399tuHs0wAAADhBZlI/5iIv/76yz799FPXXybjxSQBAAASKsiMGjUq0+kPPPCA7dy583CXCQAAIO/7yHTt2pXrLAEAAD+DzPLly6148eK5OUsAAIDcbVrq3LlzuvtBENhPP/1kH3/8sQ0ePDgnswQAAMibIFOuXLl09wsWLGj169d3V8Ru165dTmYJAACQN0Fm2rRpOXkZAABA/INMRHJysn311Vfu/40bN7aTTjopt5YLAAAgnCCzbds2u/LKK+2dd96x8uXLu2kpKSnuRHmzZ8+2o446KiezBQAACH/U0q233mq///67ffHFF/bbb7+5m06Gl5qaarfddltOZgkAAJA3NTLz58+3RYsWWcOGDaPTGjVqZOPHj6ezLwAASOwamX379lmRIkX2m65pegwAACBhg8y5555rt99+u23evDk6bdOmTdavXz9r06ZNbi4fAABA7gaZcePGuf4wtWvXtmOPPdbd6tSp46aNHTs2J7MEAADImz4yNWrUsJUrV7p+Ml9//bWbpv4ybdu2zcnsAAAAwq+Refvtt12nXtW8FChQwM477zw3gkm3U0891Z1L5t13383ZkgAAAIQZZEaPHm29evWysmXLZnrZghtvvNFGjhx5qMsAAAAQfpBZtWqVnX/++Vk+rqHXOtsvAABAwgWZrVu3ZjrsOqJw4cL2888/58ZyAQAA5G6QOfroo90ZfLPy2WefWbVq1Q5llgAAAHkTZP75z3/a4MGDbffu3fs99ueff9r9999vF154Yc6XBgAAIKzh14MGDbKXXnrJjj/+eLvlllusfv36brqGYOvyBHv37rV77733UGYJAACQN0EmKSnJli1bZr1797aBAwdaEARuuoZit2/f3oUZPQcAACAhT4hXq1Yte+ONN2z79u22bt06F2bq1atnFSpUCGcJAQAAcvPMvqLgopPgAQAAeHWtJQAAgERAkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFsEGQAAcOQNvwaOZM0HzAht3snDu4U2bwDIb6iRAQAA3iLIAAAAbyVMkHnsscfcNZv69u0bnaarbPfp08cqVapkpUuXtksvvdS2bt0a1+UEAACJIyGCzEcffWRPPfWUNW3aNN30fv362auvvmpz5syxpUuX2ubNm61z585xW04AAJBY4h5kdu7caV26dLEpU6aku/Dkjh077Omnn7aRI0faueeea82bN7dp06a5q29/8MEHcV1mAACQGOIeZNR01KFDB2vbtm266cnJyfbXX3+lm96gQQOrWbOmLV++PA5LCgAAEk1ch1/Pnj3bVq5c6ZqWMtqyZYsVLVrUypcvn256UlKSeywraWlp7haRmpqay0sNAADsSK+R2bhxo91+++02c+ZMK168eK7Nd+jQoVauXLnorUaNGrk2bwAAkFjiFmTUdLRt2zY7+eSTrXDhwu6mDr1jxoxx/1fNy549eywlJSXd6zRqqWrVqlnOd+DAga5/TeSmwAQAAPKnuDUttWnTxj7//PN006699lrXD+auu+5yNSlFihSxxYsXu2HXsmbNGvvhhx+sVatWWc63WLFi7gYAAPK/uAWZMmXK2AknnJBuWqlSpdw5YyLTr7vuOuvfv79VrFjRypYta7feeqsLMaeddlqclhoAACSShL7W0qhRo6xgwYKuRkYdeNu3b28TJkyI92IBAIAEkVBB5p133kl3X52Ax48f724AAAAJdx4ZAACAnCLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbCXXRSABA/tN8wIzQ5p08vFto84YfqJEBAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWw6/hPYZ2AsCRixoZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWQQYAAHiLIAMAALxFkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAeIsgAwAAvEWQAQAA3iLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWQQYAAHiLIAMAALxFkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFuF470AAADktuYDZoQ27+Th3UKbNw4dNTIAAMBbBBkAAOCtuAaZoUOH2qmnnmplypSxKlWqWKdOnWzNmjXpnrN7927r06ePVapUyUqXLm2XXnqpbd26NW7LDAAAEkdcg8zSpUtdSPnggw9s4cKF9tdff1m7du1s165d0ef069fPXn31VZszZ457/ubNm61z587xXGwAAJAg4trZd/78+enuP/PMM65mJjk52c466yzbsWOHPf300zZr1iw799xz3XOmTZtmDRs2dOHntNNOi9OSAwCARJBQfWQUXKRixYrurwKNamnatm0bfU6DBg2sZs2atnz58rgtJwAASAwJM/x637591rdvXzv99NPthBNOcNO2bNliRYsWtfLly6d7blJSknssM2lpae4WkZqaGvKSAwAAO9JrZNRXZvXq1TZ79uzD7kBcrly56K1GjRq5towAACCxJESQueWWW+y1116zJUuW2DHHHBOdXrVqVduzZ4+lpKSke75GLemxzAwcONA1UUVuGzduDH35AQDAERhkgiBwIWbu3Ln29ttvW506ddI93rx5cytSpIgtXrw4Ok3Ds3/44Qdr1apVpvMsVqyYlS1bNt0NAADkT4Xj3ZykEUkvv/yyO5dMpN+LmoRKlCjh/l533XXWv39/1wFYoeTWW291IYYRSwAAIK5BZuLEie7vP/7xj3TTNcS6R48e7v+jRo2yggULuhPhqRNv+/btbcKECXFZXgAAkFgKx7tp6UCKFy9u48ePdzcAAICE6+wLAACQEwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC34npCPAAA8ovmA2aEOv/k4d1Cnb+vqJEBAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAAOAtggwAAPAWQQYAAHiLIAMAALxFkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAeIsgAwAAvEWQAQAA3iLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeKhzvBUD+0XzAjFDnnzy8W6jzBwD4hxoZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3OI8MAACeas75u6iRAQAA/iLIAAAAbxFkAACAtwgyAADAWwQZAADgLYIMAADwFsOv8ymG5AEAjgTUyAAAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAeIsgAwAAvEWQAQAA3iLIAAAAbxFkAACAt7hEgeeXC+BSAQCAIxk1MgAAwFsEGQAA4C0vgsz48eOtdu3aVrx4cWvZsqWtWLEi3osEAAASQML3kXnhhResf//+NmnSJBdiRo8ebe3bt7c1a9ZYlSpVDnl+9FcBAODwJNK+NOFrZEaOHGm9evWya6+91ho1auQCTcmSJW3q1KnxXjQAABBnCR1k9uzZY8nJyda2bdvotIIFC7r7y5cvj+uyAQCA+EvopqVffvnF9u7da0lJSemm6/7XX3+d6WvS0tLcLWLHjh3ub2pqqvu7N+3P0JY3UkZG+a3MrMqNR5lhl0uZ+avMrMrlu0uZiV7mkfjdTf3//wZBkP0LggS2adMmLX2wbNmydNMHDBgQtGjRItPX3H///e413Lhx48aNGzfz/rZx48Zss0JC18hUrlzZChUqZFu3bk03XferVq2a6WsGDhzoOgdH7Nu3z3777TerVKmSFShQ4KDLVhKsUaOGbdy40cqWLWt5JR7lUmb+KjNe5VImZfpaLmUmZpmqifn999+tevXq2T4voYNM0aJFrXnz5rZ48WLr1KlTNJjo/i233JLpa4oVK+ZuscqXL5/jZdCKz8sfcTzLpcz8VWa8yqVMyvS1XMpMvDLLlSt3wOckdJAR1a50797dTjnlFGvRooUbfr1r1y43igkAABzZEj7IXHHFFfbzzz/bfffdZ1u2bLFmzZrZ/Pnz9+sADAAAjjwJH2REzUhZNSWFRc1T999//37NVPmxXMrMX2XGq1zKpExfy6VMv8ssoB6/oc0dAADgSD0hHgAAQHYIMgAAwFsEGQAA4C2CDAAA8BZBJgvjx4+32rVrW/Hixa1ly5a2YsWKPCv7sccec2ch7tu3b6jlPPDAA66c2FuDBg0sbJs2bbKuXbu6sy2XKFHCmjRpYh9//HFo5elzzPg+devTp09oZeoaYYMHD7Y6deq493jsscfaQw89dOBrhhwmnQVT35tatWq5clu3bm0fffRRrs3/f//7n1100UXuTJtah/PmzUv3uN6fTpVQrVo1V74u8PrNN9+EXu5LL71k7dq1i57B+9NPPw21zL/++svuuusu990tVaqUe063bt1s8+bNoZUZ+c3qN6oyK1So4Nbvhx9+GGqZsW666Sb3HJ3PK8wye/Tosd/v9fzzzw+1TPnqq6/s4osvdidh0zo+9dRT7Ycffgi13My2TboNHz48tDJ37tzpRgIfc8wx7nfaqFEjmzRpUo7LO5gydUZ+fa56vGTJku7zzI1tgxBkMvHCCy+4E/FpyNjKlSvtxBNPtPbt29u2bdtCL1s7naeeesqaNm1qeaFx48b2008/RW/vvfdeqOVt377dTj/9dCtSpIi9+eab9uWXX9qIESPcRjnMdRr7HhcuXOimX3bZZaGVOWzYMJs4caKNGzfObRx1//HHH7exY8damK6//nr3/p599ln7/PPP3c5dOzuFx9ygk1Hq96Cgnxm9xzFjxriNonaw2hnot7N79+5Qy9XjZ5xxhlvPuSW7Mv/44w+3bVBY1V8FqTVr1ridYFhlyvHHH+++U/ps9VtVSNdnrHNthVVmxNy5c+2DDz444Onic6tM7ehif7fPP/98qGV+++237jukoPjOO+/YZ5995j5fHcyGWW7se9Rt6tSpLghceumloZXZv39/dz625557zm2fdPCjYPPKK6+EUqYOcHR2/u+++85efvll++STT9zBlrZNet1hy82LPOYXuiBlnz59ovf37t0bVK9ePRg6dGio5f7+++9BvXr1goULFwZnn312cPvtt4dani6weeKJJwZ56a677grOOOOMIJ60Xo899thg3759oZXRoUOHoGfPnummde7cOejSpUtoZf7xxx9BoUKFgtdeey3d9JNPPjm49957c708bT7mzp0bva/1WbVq1WD48OHRaSkpKUGxYsWC559/PrRyY61fv949/sknn+RaeQcqM2LFihXueRs2bMizMnfs2OGet2jRolDL/PHHH4Ojjz46WL16dVCrVq1g1KhRuVJeVmV279496NixY66VcTBlXnHFFUHXrl1DKzOrcjPS+z733HNDLbNx48bBgw8+GNp2ImOZa9ascdP0/Yndrx511FHBlClTDrs8amQy2LNnjyUnJ7ukGFGwYEF3f/ny5aGWraaODh06pCs7bKra0xFW3bp1rUuXLoddjXogSvy63IRqQ6pUqWInnXSSTZkyxfLy89VRSM+ePQ/pIqKHSk06uibY2rVr3f1Vq1a5I+gLLrggtDL//vtv16SV8QhSVcdh17TJ+vXr3dm3Y7+/qqJX02zYv51EsGPHDvedOpxrux3qd3ny5MluHetIOCy6vt0111xjAwYMcDW4eUW1ItpG1K9f33r37m2//vprqO/x9ddfdzVeqkFUufreZtfMFgY1v2g5rrvuulDLad26tdsWq6ZWuWPJkiVuW6XavTCkpaW5v7HbJu1XdZK83Ng2EWQy+OWXX9zOIOMlEHRfG+mwzJ4921VRDx061PKKfqjPPPOMq2JUM4h2RGeeeabrZxEWVS2qrHr16tmCBQvcBuq2226z6dOnW17QhiklJcW11Ybp7rvvtiuvvNJVU6sZTYFN1bcKi2EpU6aMtWrVyvXFUV8NfY8V2hQiVGUdtsjvI69/O4lATWfqM3PVVVeFfjG+1157zUqXLu12CqNGjXJNiZUrVw6tPDXXFS5c2P1O84qalWbMmOEOBlT+0qVL3UGAvtNhULcB9RtR/0SV/dZbb9kll1xinTt3dmXnFW0H9TtWuWEaO3as6xejPjK6OLPes5qEzjrrrFDK03awZs2aNnDgQNe9QCFcn+uPP/6YK9smLy5RkN/p8ua333672yAdbnvsoYitHVCfHAUbtVu++OKLoR0R6MhHNTKPPvqou68d/OrVq12fCl0cNGxPP/20e9+50c6fHa3DmTNn2qxZs9xRrDqfKsio3DDfp/rGqLbp6KOPtkKFCtnJJ5/sdq6qZUQ41PH38ssvd0e2CulhO+ecc9z3SQddqs1U2eqPpFqE3KbvzZNPPukOssKswcxIBwER6lCt7ZM6zKuWpk2bNqFsl6Rjx47Wr18/939d12/ZsmVu23T22WdbXlD/GB3shL0fGDt2rOvvpFoZbfPVUVctAto+hdEioIM59SPTfqVixYpu26RytC3OjQEQ1MhkoCMbrWRV8cXS/apVq4ZSpjYWOiLQTkdHPrrpKECdJvX/sI5CMlKVuKpW161bF1oZGs2iI4FYDRs2DL1JSzZs2GCLFi1yHWLDpmr4SK2MNsSqmtcGMuwaN23s9d3R0aUCskbbaUerpsOwRX4fefnbSZQQo++WDkTCro0RdaA+7rjj7LTTTnPBXNsI/Q3Du+++67ZNOpqObJv0Xv/973+7jsZ5Rd9fbZvD2jZp3npv8do2Rda1OoyHvX36888/7Z577rGRI0e6UUYKieroqws0P/HEE6GV27x5cxfAVSOuWhi1BKi5MDe2TQSZDFTNphWuKs3YtK77qrYPg44wNApBH3LkploLJXP9X8EqL2jnp577Chth0Ygl/VhjqW1WRwVhmzZtmjtqVT+ksGlUi9qAY+lzjBz55cXOTp+jqnHVhKcjzbBpqLkCS+xvJzU11dUWhPXbSYQQo35mCsga+h0P+k5F+iDkNgVwjd6J3TbpqF1BXd+rvKImCO30wto2abuvodbx2jaJwqj2PWH2d4p8b3WL1/ZJfbqOOuoo97vRaTdyY9tE01IWQ9NU/a8w0aJFC3fOBA0Ru/baa0MpT22iJ5xwwn47Im0YM07PTXfccYdL5Pqhqk+Fhpvry6ymiLCoVkIdzdS0pJ2AagzUYVG3MOkHqiCjz1VHXmHTen3kkUfckayaljTcUEdAavYJk3YuqqpVB0kdvWqHo/bp3PruKuzGHhWrX5V2bqou1ntV89nDDz/s+kAp2Gj4qnZ8GnoZZrm//fabO3KOnMclskNSsMppbVB2ZWqH+q9//cs1uajPimpNI/2A9Lh2jLldprYH+k5piLfKV9OS+jWow+bhnErgQOs2Y0BTM4HWqb5jYZSp25AhQ9zwY5Wjg6s777zT1UKpI25Y71O/FdVKqJ+Imu9UY/Dqq6+65qzDcaByI4F/zpw57lQUuWHnAcpUU5nerwYCaPuvWlz1SdI2Kqwy9f4UYPR/HbirO4W2C7nSwfiwxz3lU2PHjg1q1qwZFC1a1A3H/uCDD/K0/LwYfq3hhtWqVXPvUUMrdX/dunVB2F599dXghBNOcMNyGzRoEEyePDn0MhcsWOCG/2kYYF5ITU11n5++Q8WLFw/q1q3rhjampaWFWu4LL7zgytJnqqHQOo2AhkDnliVLlrj1mPGm4bKRIdiDBw8OkpKS3Ofbpk2bXFnnByp32rRpmT6uUwyEUWZkmHdmN70ujDL//PPP4JJLLnGngtDnq9/uxRdf7IZ9h7luM8qN4dfZlanTCLRr184NzS1SpIgrr1evXsGWLVtCKzPi6aefDo477jj3m9WpKebNm3dYZR5suU899VRQokSJXPutLjlAmT/99FPQo0cP913Se61fv34wYsSIwzolxYHKfPLJJ4NjjjnGfabaLg4aNCjXtocF9M/hxyEAAIC8Rx8ZAADgLYIMAADwFkEGAAB4iyADAAC8RZABAADeIsgAAABvEWQAAIC3CDIAAMBbBBkAcdOjR4/DvnzBgegaPbrG1vfff2/xpssKaFl07SAAuYMgA8DRtYJ0/RNd06Z48eKWlJTkLvI5ceJEdxFMX+n6RLowXezVmnVdJl08tGTJki5Y6Lozf//9d7bzmTJlip155plWoUIFd2vbtq27VlgsnSj9vvvuc9dC0nVs9BxdHC/2KsvdunVz1zUDkDsIMgDsu+++s5NOOsneeustd0FPXeRy+fLl7mJ9uiiiru6cFV1JN1EpgOmqwtddd110mi7wqBCzZ88eW7ZsmU2fPt2eeeYZF0Cyo4sH6oKqS5YsceumRo0a7oJ3umhjxOOPP25jxoyxSZMmuat+6+KvutDh7t27o8/RBTxnzpzpLnQJIBfkyhWbAHitffv27oJuO3fuzPTx2IvJabMxYcKE4KKLLgpKlizpLsz4999/Bz179gxq167tLkJ3/PHHB6NHj043Dz2nX79+Qbly5YKKFSsGAwYMCLp16xZ07Ngx+py9e/cGjz76aHQ+TZs2DebMmRN9/LfffguuvvrqoHLlyu5xXeBv6tSpWb4vvVYXH4z1xhtvBAULFkx3EcKJEycGZcuWPaSL2On9lClTJpg+fXp0HelCncOHD48+RxcB1MUzn3/++XSvrVOnTvCf//znoMsCkDVqZIAjnPqQqCamT58+rgYhMwUKFEh3/4EHHrBLLrnEPv/8c+vZs6ft27fPjjnmGJszZ459+eWXrnbjnnvusRdffDH6mhEjRriaj6lTp9p7773naiTmzp2bbr5Dhw61GTNmuBqNL774wvr162ddu3a1pUuXuscHDx7s5v/mm2/aV1995Zq91FyTlXfffdeaN2+ebppqU5o0aeKaziJUa5KamurKPJTaHtVGVaxY0d1fv369a55Tc1JEuXLlrGXLlq7MWC1atHDLBuDwFc6FeQDw2Lp161zfjvr166ebroAQaRJRyBk2bFj0sauvvto1kcQaMmRI9P916tRxO28Fmcsvv9xNGz16tA0cONA6d+7s7iusLFiwIPqatLQ016ylZqxWrVq5aXXr1nWh56mnnrKzzz7b9W1RE9gpp5ziHo/t95KZDRs2WPXq1dNNU9iIDTESua/HDtZdd93l5h0JLpHXZjbvjPPV69R8B+DwUSMDIFPqyPrpp59a48aNXciIFQkSscaPH+9qP4466igrXbq0TZ482QUP2bFjh/3000+udiKicOHC6eajQKVajvPOO8+9PnJTDc23337rntO7d2+bPXu2NWvWzPXfUR+X7Pz555+u4/Kh0DLHlq9wldFjjz3mlkM1Soc6f1FHYJ87UAOJhBoZ4AinUUpqOlqzZk266aoNiex0M8rYBKWd+h133OGaj1SbUqZMGRs+fLjr8Hqwdu7c6f6+/vrrdvTRR6d7rFixYu7vBRdc4GpZ3njjDVu4cKG1adPG1RY98cQTmc5TtUrbt29PN61q1ar7jTbaunVr9DHVlijARUSajiJUloKMao6aNm2abr6ReWnUUuy8FbxiqVlNgQ/A4aNGBjjCVapUydWCjBs3znbt2pWjebz//vvWunVru/nmm13Tj8JRpBYl0ldEO/fYYKPhzsnJydH7jRo1coFFNSJ6fexNI4QiFAC6d+9uzz33nGuuUs1PVrQs6lMTS0FLfXu2bdsWnaZQVLZsWbcMqimKLTs2yGhU0kMPPWTz58/fr1ZKzWkKM4sXL45OU78bvedIU1nE6tWr3bIBOHwEGQA2YcIEFyy0c37hhRdcR1rV0CgsfP3111aoUKFsX1+vXj37+OOPXZ+XtWvXuk65H330Ubrn6Bw1qsmYN2+em6dCT0pKSvRx1eKoVkcdfDUkWkFo5cqVNnbsWHdf1In45Zdfds1Q6piroeENGzbMcrnUiVfPi62V0ZBpBZZrrrnGVq1a5ZZ50KBBrmYnUvOTGfUR0vtSZ2X1zVG/F90iNUmq1erbt689/PDD9sorr7iwpHPGqIYn9qR/alJSgNNyAMgF2YxoAnAE2bx5c3DLLbe4ocFFihQJSpcuHbRo0cINJ961a1f0edpszJ07N91rd+/eHfTo0cMNrS5fvnzQu3fv4O677w5OPPHE6HP++uuv4Pbbb3fDnPWc/v377zf8WkOYNWy7fv36bhk0dFpDw5cuXeoef+ihh4KGDRsGJUqUcEO49drvvvsu2/el9zBp0qR0077//vvgggsucPPRUO5///vfbvmyU6tWLffeM940/Dx2+QcPHhwkJSW5Yddt2rQJ1qxZk24+s2bNcu8PQO4ooH9yIxABQCJSnxuduVfNOQULxr8S+rTTTrPbbrvNjfwCcPjo7AsgX9NZfHWZAJ2BN7avTbyutaTh5zpDMIDcQY0MAADwVvzrWQEAAHKIIAMAALxFkAEAAN4iyAAAAG8RZAAAgLcIMgAAwFsEGQAA4C2CDAAA8BZBBgAAmK/+H3+8l+uL2jk2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='G1', data=df)\n",
    "plt.title('Grade distribution for course 1')\n",
    "plt.xlabel('Grades (0-20)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bfbb41",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis: Grade Distribution\n",
    "\n",
    "### Why Visualize the Target Variable?\n",
    "\n",
    "Understanding the distribution of grades helps us:\n",
    "1. **Identify class imbalance**: Are passes/fails evenly distributed?\n",
    "2. **Choose appropriate threshold**: Where to draw the line between pass/fail?\n",
    "3. **Set baseline expectations**: What accuracy could we get by always guessing the majority class?\n",
    "\n",
    "### Seaborn Countplot\n",
    "\n",
    "**`sns.countplot(x='G1', data=df)`**:\n",
    "- Creates a bar chart showing the frequency of each unique value\n",
    "- X-axis: Different grade values (0-20)\n",
    "- Y-axis: Count of students with that grade\n",
    "- Helps visualize if grades are normally distributed, skewed, or bimodal\n",
    "\n",
    "### What to Look For:\n",
    "- **Central tendency**: Where do most grades cluster?\n",
    "- **Outliers**: Are there very low or very high grades?\n",
    "- **Distribution shape**: Normal, uniform, or skewed?\n",
    "- **Gaps**: Are there grade values with zero students?\n",
    "\n",
    "This visualization informs our threshold selection for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31146e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df.rename(columns={'G1': 'period_1_grades', 'G2': 'period_2_grades', 'G3': 'final_grade'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111a89e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "passed",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "71777b7c-1069-49a1-afff-2eeeb67c329d",
       "rows": [
        [
         "True",
         "549"
        ],
        [
         "False",
         "100"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "passed\n",
       "True     549\n",
       "False    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ALTERNATIVE 1\n",
    "\n",
    "# Define the threshold for passing\n",
    "passing_grade = 10  # Adjust the threshold as needed (assuming grades are out of 20)\n",
    "\n",
    "# Create the 'passed' column\n",
    "df['passed'] = df['final_grade'] >= passing_grade\n",
    "\n",
    "\n",
    "# check the value counts\n",
    "df['passed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc3225",
   "metadata": {},
   "source": [
    "## 4. Creating Binary Target Variable: Pass/Fail Classification\n",
    "\n",
    "### Why Binary Classification?\n",
    "Original grades (0-20) represent a **regression problem** (predicting continuous values).\n",
    "Converting to pass/fail creates a **binary classification problem** (predicting one of two classes).\n",
    "\n",
    "**Benefits:**\n",
    "- Simpler problem to solve\n",
    "- More interpretable results (pass or fail, not 12.7)\n",
    "- Often more useful for decision-making (\"Will this student pass?\")\n",
    "\n",
    "### Threshold Selection: Grade ≥ 10\n",
    "\n",
    "**Why 10?**\n",
    "- Common passing grade in Portuguese education system (50%)\n",
    "- Represents basic competency\n",
    "- Balances the classes reasonably well\n",
    "\n",
    "### Two Implementation Methods\n",
    "\n",
    "**Alternative 1: Boolean Comparison**\n",
    "```python\n",
    "df['passed'] = df['final_grade'] >= passing_grade\n",
    "```\n",
    "- Uses pandas vectorized operations\n",
    "- Returns boolean Series (True/False)\n",
    "- Efficient and readable\n",
    "\n",
    "**Alternative 2: Lambda Function**\n",
    "```python\n",
    "df['passed'] = df['final_grade'].apply(lambda x: True if x >= 10 else False)\n",
    "```\n",
    "- Uses `apply()` with lambda function\n",
    "- More explicit conditional logic\n",
    "- Same result, slightly less efficient\n",
    "\n",
    "**Both methods are valid!** Alternative 1 is more \"pythonic\" and faster.\n",
    "\n",
    "### Class Distribution Check\n",
    "`df['passed'].value_counts()` shows how many students passed vs. failed.\n",
    "- Important for understanding class imbalance\n",
    "- Affects baseline accuracy and model evaluation strategy\n",
    "- Severe imbalance (e.g., 95%-5%) may require special techniques (SMOTE, class weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa12e259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "passed",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "52ffa918-cb5f-490a-942e-f594b24929b8",
       "rows": [
        [
         "True",
         "549"
        ],
        [
         "False",
         "100"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "passed\n",
       "True     549\n",
       "False    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTERNATIVE 2\n",
    "# Create binary target variable\n",
    "df['passed'] = df['final_grade'].apply(lambda x: True if x >= 10 else False)\n",
    "\n",
    "# check the value counts\n",
    "df['passed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2052588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of people who passed with threshold 10:  84.59167950693374\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of people who passed with threshold 10: ', 549/(549+100)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9db3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'period_1_grades', 'period_2_grades', 'final_grade']\n"
     ]
    }
   ],
   "source": [
    "# Select numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numerical columns:\", numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd40bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "age                0\n",
      "Medu               0\n",
      "Fedu               0\n",
      "traveltime         0\n",
      "studytime          0\n",
      "failures           0\n",
      "famrel             0\n",
      "freetime           0\n",
      "goout              0\n",
      "Dalc               0\n",
      "Walc               0\n",
      "health             0\n",
      "absences           0\n",
      "period_1_grades    0\n",
      "period_2_grades    0\n",
      "passed             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical columns\n",
    "numerical_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures',\n",
    "                  'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health',\n",
    "                  'absences', 'period_1_grades', 'period_2_grades']\n",
    "\n",
    "# Create a new DataFrame with only numerical features and the target variable\n",
    "df_numeric = df[numerical_cols + ['passed']]\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df_numeric.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf6732",
   "metadata": {},
   "source": [
    "## 5. Feature Selection: Numerical Features\n",
    "\n",
    "### Why Only Numerical Features?\n",
    "\n",
    "The student dataset contains both **numerical** and **categorical** features.\n",
    "\n",
    "**Numerical features** (already numbers):\n",
    "- `age`: 15-22 years\n",
    "- `Medu`, `Fedu`: Parent education (0-4)\n",
    "- `studytime`: Weekly study time (1-4)\n",
    "- `failures`: Number of past class failures (0-4)\n",
    "- `absences`: Number of school absences (0-93)\n",
    "- `period_1_grades`, `period_2_grades`: Previous grades (0-20)\n",
    "\n",
    "**Categorical features** (text labels):\n",
    "- `sex`: M/F\n",
    "- `school`: GP/MS\n",
    "- `address`: U/R (urban/rural)\n",
    "- `Pstatus`: T/A (together/apart)\n",
    "\n",
    "### Why Start with Numerical?\n",
    "\n",
    "1. **Simplicity**: Decision Trees, Random Forests, and Naive Bayes can work with numbers directly\n",
    "2. **Learning focus**: Understand algorithms before tackling encoding complexity\n",
    "3. **Many algorithms prefer numbers**: SVM, KNN, Neural Networks need numerical input\n",
    "\n",
    "### Handling Categorical Features (Not Done Here)\n",
    "\n",
    "For complete analysis, you would:\n",
    "- **Label Encoding**: Convert M/F → 0/1 (ordinal relationships)\n",
    "- **One-Hot Encoding**: Convert school (GP/MS) → two binary columns [school_GP, school_MS]\n",
    "- Decision Trees handle categoricals well, but encoding often improves performance\n",
    "\n",
    "### Missing Values Check\n",
    "\n",
    "`df_numeric.isnull().sum()` checks each column for missing data.\n",
    "- **Missing data** can break many algorithms\n",
    "- **Strategies**: Remove rows, fill with mean/median, or use algorithms that handle missing values\n",
    "- This dataset is clean (no missing values), but always check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe94c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df_numeric.drop(columns=['passed'])\n",
    "y = df_numeric['passed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e50bac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116dfe9",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split\n",
    "\n",
    "### Why Split the Data?\n",
    "\n",
    "**The Fundamental Problem of Machine Learning:**\n",
    "We want models that **generalize** to new, unseen data.\n",
    "\n",
    "**Training on all data** → **Overfitting Risk**\n",
    "- Model memorizes the training examples\n",
    "- Performs perfectly on training data\n",
    "- Fails on new data\n",
    "\n",
    "**Solution:** Hold out a test set to evaluate generalization performance.\n",
    "\n",
    "### The Split: 70% Train, 30% Test\n",
    "\n",
    "**`train_test_split(X, y, test_size=0.3, random_state=42)`**\n",
    "\n",
    "**Parameters:**\n",
    "- `test_size=0.3`: 30% of data for testing, 70% for training\n",
    "- `random_state=42`: Seed for reproducibility\n",
    "  - Without this, you get different splits each run\n",
    "  - 42 is arbitrary (but famous from \"Hitchhiker's Guide to the Galaxy\")\n",
    "  - Using the same seed ensures identical splits across experiments\n",
    "\n",
    "**Returns:**\n",
    "- `X_train`: Features for training (70% of data)\n",
    "- `X_test`: Features for testing (30% of data)\n",
    "- `y_train`: Target labels for training\n",
    "- `y_test`: Target labels for testing\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**Common Split Ratios:**\n",
    "- 80/20 (more training data)\n",
    "- 70/30 (balance between training and evaluation)\n",
    "- 60/20/20 (train/validation/test for hyperparameter tuning)\n",
    "\n",
    "**Important:**\n",
    "- **Never** train on test data\n",
    "- **Never** tune hyperparameters based on test performance\n",
    "- Test set should only be used for **final** evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25c9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9230769230769231\n",
      "Confusion Matrix:\n",
      "[[ 19   7]\n",
      " [  8 161]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Decision Tree classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "  \n",
    "# Train the model\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "cm = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219a553",
   "metadata": {},
   "source": [
    "## 7. Decision Tree Classifier\n",
    "\n",
    "### What is a Decision Tree?\n",
    "\n",
    "A **decision tree** is a flowchart-like structure where:\n",
    "- Each **internal node** represents a test on a feature (e.g., \"is studytime >= 3?\")\n",
    "- Each **branch** represents the outcome of the test\n",
    "- Each **leaf node** represents a class label (passed/failed)\n",
    "\n",
    "### How Decision Trees Make Decisions\n",
    "\n",
    "**Example Decision Path:**\n",
    "```\n",
    "Start → Is period_2_grades >= 12? \n",
    "        ├─ Yes → Is absences <= 5?\n",
    "        │        ├─ Yes → Predict: PASSED ✓\n",
    "        │        └─ No → Predict: FAILED ✗\n",
    "        └─ No → Predict: FAILED ✗\n",
    "```\n",
    "\n",
    "### How Trees Are Built: Recursive Partitioning\n",
    "\n",
    "**Algorithm (CART - Classification and Regression Trees):**\n",
    "\n",
    "1. **Start with all training data at root**\n",
    "2. **For each feature**, find the best split that:\n",
    "   - Maximizes **information gain** (or minimizes **Gini impurity**)\n",
    "   - Best separates the classes\n",
    "3. **Split** the data based on that feature/threshold\n",
    "4. **Recursively repeat** steps 2-3 for each child node\n",
    "5. **Stop** when:\n",
    "   - All samples in a node have the same class (pure node)\n",
    "   - Maximum depth reached\n",
    "   - Minimum samples per node reached\n",
    "\n",
    "### Splitting Criteria\n",
    "\n",
    "**Gini Impurity** (default in sklearn):\n",
    "$$\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "\n",
    "- Measures \"impurity\" of a node\n",
    "- $p_i$ = proportion of class $i$\n",
    "- Gini = 0 → perfectly pure (all one class)\n",
    "- Gini = 0.5 → maximally impure (50/50 split for binary)\n",
    "\n",
    "**Information Gain** (based on entropy):\n",
    "$$\\text{Entropy} = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "\n",
    "- From information theory\n",
    "- Measures uncertainty/disorder\n",
    "- Tree tries to maximize information gain at each split\n",
    "\n",
    "### Advantages of Decision Trees\n",
    "\n",
    "✅ **Highly Interpretable**: Can visualize and explain decisions\n",
    "✅ **No Feature Scaling Needed**: Works with raw numerical values\n",
    "✅ **Handles Non-Linear Relationships**: Can capture complex patterns\n",
    "✅ **Feature Importance**: Automatically ranks feature importance\n",
    "✅ **Works with Mixed Data**: Can handle numerical and categorical\n",
    "\n",
    "### Disadvantages of Decision Trees\n",
    "\n",
    "❌ **Overfitting**: Tends to create overly complex trees that memorize training data\n",
    "❌ **High Variance**: Small changes in data → completely different tree\n",
    "❌ **Biased to Dominant Classes**: Can ignore minority classes\n",
    "❌ **Not Optimal**: Greedy algorithm doesn't guarantee global optimum\n",
    "❌ **Unstable**: Small data perturbations can change tree structure significantly\n",
    "\n",
    "### Controlling Overfitting\n",
    "\n",
    "**Hyperparameters to tune:**\n",
    "- `max_depth`: Maximum tree depth (deeper = more complex)\n",
    "- `min_samples_split`: Minimum samples required to split a node\n",
    "- `min_samples_leaf`: Minimum samples required in a leaf node\n",
    "- `max_features`: Number of features to consider for best split\n",
    "\n",
    "### The Code Explained\n",
    "\n",
    "```python\n",
    "dt = DecisionTreeClassifier()  # Initialize with default parameters\n",
    "dt.fit(X_train, y_train)       # Build tree from training data\n",
    "y_pred_dt = dt.predict(X_test) # Make predictions on test data\n",
    "```\n",
    "\n",
    "**Without hyperparameter tuning**, the tree can grow very deep and overfit!\n",
    "Better approach: Use GridSearchCV to find optimal `max_depth`, `min_samples_split`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f65e073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9384615384615385\n",
      "Confusion Matrix:\n",
      "[[ 18   8]\n",
      " [  4 165]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed5272",
   "metadata": {},
   "source": [
    "## 8. Random Forest Classifier\n",
    "\n",
    "### What is a Random Forest?\n",
    "\n",
    "A **Random Forest** is an **ensemble** of decision trees.\n",
    "\n",
    "**Ensemble Learning**: Combine multiple models to get better performance than any single model.\n",
    "\n",
    "**Analogy**: \"Wisdom of crowds\"\n",
    "- 1 expert can be wrong\n",
    "- 100 experts voting together → more reliable\n",
    "\n",
    "### How Random Forests Work\n",
    "\n",
    "**Training Process:**\n",
    "\n",
    "1. **Bootstrap Sampling (Bagging)**:\n",
    "   - Create multiple training sets by sampling with replacement\n",
    "   - Each tree trains on a different random subset (~63% of original data)\n",
    "   - This is called **Bootstrap AGGregatING** = **Bagging**\n",
    "\n",
    "2. **Random Feature Selection**:\n",
    "   - At each split, only consider a random subset of features\n",
    "   - Default: $\\sqrt{n_{features}}$ for classification\n",
    "   - For this dataset: $\\sqrt{15} \\approx 4$ features per split\n",
    "   - **Why?** Decorrelates trees → reduces overall variance\n",
    "\n",
    "3. **Build Many Trees**:\n",
    "   - Typically 100-1000 trees (default = 100)\n",
    "   - Each tree is fully grown (no pruning)\n",
    "   - Each tree is different due to random sampling\n",
    "\n",
    "**Prediction Process:**\n",
    "\n",
    "For classification:\n",
    "- Each tree votes for a class\n",
    "- **Majority vote wins**\n",
    "- Example: 60 trees vote \"Passed\", 40 vote \"Failed\" → Predict \"Passed\"\n",
    "\n",
    "For regression:\n",
    "- Average the predictions of all trees\n",
    "\n",
    "### Why Random Forests Are Powerful\n",
    "\n",
    "**Variance Reduction:**\n",
    "$$\\text{Variance}_{\\text{forest}} = \\frac{\\text{Variance}_{\\text{tree}}}{n_{\\text{trees}}}$$\n",
    "\n",
    "- Averaging many high-variance trees → low-variance ensemble\n",
    "- Individual trees overfit, but forest generalizes well\n",
    "\n",
    "**The Bias-Variance Trade-off:**\n",
    "- **Single Decision Tree**: Low bias, **high variance** → overfits\n",
    "- **Random Forest**: Low bias, **low variance** → generalizes well\n",
    "- **Best of both worlds!**\n",
    "\n",
    "### Advantages of Random Forests\n",
    "\n",
    "✅ **Excellent Performance**: Often best \"out-of-the-box\" algorithm\n",
    "✅ **Reduces Overfitting**: Averaging many trees reduces variance\n",
    "✅ **Robust**: Handles outliers and noise well\n",
    "✅ **Feature Importance**: Can rank feature importance\n",
    "✅ **Parallel Training**: Trees are independent → can train in parallel\n",
    "✅ **No Feature Scaling Needed**: Like decision trees\n",
    "✅ **Handles Missing Data**: Can estimate missing values\n",
    "\n",
    "### Disadvantages of Random Forests\n",
    "\n",
    "❌ **Less Interpretable**: Can't visualize 100 trees easily\n",
    "❌ **Slower Prediction**: Must query 100+ trees (vs 1 tree)\n",
    "❌ **Memory Intensive**: Stores many large trees\n",
    "❌ **Not Great for Extrapolation**: Can't predict beyond training range\n",
    "❌ **Biased with Imbalanced Data**: Majority class can dominate\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "**`n_estimators`** (default=100):\n",
    "- Number of trees in the forest\n",
    "- More trees → better performance (up to a point)\n",
    "- More trees → slower training/prediction\n",
    "- Typical range: 100-1000\n",
    "\n",
    "**`max_depth`**:\n",
    "- Maximum depth of each tree\n",
    "- Default: None (trees grow until pure leaves)\n",
    "- Limiting depth can prevent overfitting\n",
    "\n",
    "**`min_samples_split`**:\n",
    "- Minimum samples required to split a node\n",
    "- Higher values → simpler trees\n",
    "\n",
    "**`max_features`**:\n",
    "- Number of features to consider at each split\n",
    "- Default: 'sqrt' → $\\sqrt{n_{features}}$\n",
    "- 'log2' is also common\n",
    "\n",
    "**`bootstrap`**:\n",
    "- Whether to use bootstrap samples\n",
    "- Default: True (this is what makes it \"bagging\")\n",
    "\n",
    "### Out-of-Bag (OOB) Error\n",
    "\n",
    "A unique feature of Random Forests:\n",
    "- Each tree trains on ~63% of data\n",
    "- Remaining ~37% is \"out-of-bag\" (OOB)\n",
    "- Can use OOB samples for validation\n",
    "- **Free cross-validation!** No need for separate validation set\n",
    "\n",
    "### The Code Explained\n",
    "\n",
    "```python\n",
    "rf = RandomForestClassifier()  # Default: 100 trees\n",
    "rf.fit(X_train, y_train)       # Train 100 trees in parallel\n",
    "y_pred_rf = rf.predict(X_test) # Each tree votes, majority wins\n",
    "```\n",
    "\n",
    "**Better approach with tuning:**\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de2680fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9076923076923077\n",
      "Confusion Matrix:\n",
      "[[ 20   6]\n",
      " [ 12 157]]\n"
     ]
    }
   ],
   "source": [
    "#init Gaussian Naive Bayes\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_nb}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7a659",
   "metadata": {},
   "source": [
    "## 9. Gaussian Naive Bayes Classifier\n",
    "\n",
    "### What is Naive Bayes?\n",
    "\n",
    "A **probabilistic classifier** based on **Bayes' Theorem** with a \"naive\" assumption.\n",
    "\n",
    "**Bayes' Theorem:**\n",
    "$$P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}$$\n",
    "\n",
    "Where:\n",
    "- $P(C|X)$ = **Posterior probability**: Probability of class $C$ given features $X$\n",
    "- $P(X|C)$ = **Likelihood**: Probability of features $X$ given class $C$\n",
    "- $P(C)$ = **Prior probability**: Overall probability of class $C$\n",
    "- $P(X)$ = **Evidence**: Probability of observing features $X$\n",
    "\n",
    "**In Plain English:**\n",
    "\"Given these features (age=18, studytime=3, absences=2), what's the probability this student passed?\"\n",
    "\n",
    "### The \"Naive\" Assumption\n",
    "\n",
    "**Conditional Independence**: All features are **independent** given the class.\n",
    "\n",
    "For multiple features $X = (x_1, x_2, ..., x_n)$:\n",
    "$$P(X|C) = P(x_1|C) \\cdot P(x_2|C) \\cdot ... \\cdot P(x_n|C)$$\n",
    "\n",
    "**Example:**\n",
    "Assumes `studytime` and `absences` are independent (knowing one doesn't tell you about the other).\n",
    "\n",
    "**Reality:** Features are often correlated!\n",
    "- Students with high studytime probably have low absences\n",
    "- Despite this violation, Naive Bayes often works surprisingly well!\n",
    "\n",
    "### Complete Formula\n",
    "\n",
    "$$P(C|X) = \\frac{P(C) \\cdot \\prod_{i=1}^{n} P(x_i|C)}{P(X)}$$\n",
    "\n",
    "**Prediction:** Choose class with highest posterior probability\n",
    "$$\\hat{C} = \\arg\\max_C P(C|X)$$\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "**GaussianNB** assumes each feature follows a **Gaussian (Normal) distribution** within each class.\n",
    "\n",
    "For each feature $i$ and class $C$:\n",
    "$$P(x_i|C) = \\frac{1}{\\sqrt{2\\pi\\sigma_C^2}} \\exp\\left(-\\frac{(x_i - \\mu_C)^2}{2\\sigma_C^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_C$ = mean of feature $i$ for class $C$\n",
    "- $\\sigma_C^2$ = variance of feature $i$ for class $C$\n",
    "\n",
    "**Training Process:**\n",
    "1. For each class (Passed/Failed)\n",
    "2. For each feature (age, studytime, absences, ...)\n",
    "3. Calculate mean ($\\mu$) and variance ($\\sigma^2$)\n",
    "4. Store these statistics\n",
    "\n",
    "**Prediction Process:**\n",
    "1. For each class, calculate $P(C|X)$ using stored $\\mu$ and $\\sigma^2$\n",
    "2. Pick class with highest probability\n",
    "\n",
    "### Other Naive Bayes Variants\n",
    "\n",
    "**MultinomialNB**:\n",
    "- For **discrete count data** (word counts in text)\n",
    "- Used in text classification, spam detection\n",
    "- Example: Email has 5 occurrences of \"money\", 3 of \"free\"\n",
    "\n",
    "**BernoulliNB**:\n",
    "- For **binary/boolean features** (presence/absence)\n",
    "- Example: Word appears in email (yes/no)\n",
    "\n",
    "**CategoricalNB** (sklearn 0.22+):\n",
    "- For **categorical features**\n",
    "- Example: School (GP/MS), Address (Urban/Rural)\n",
    "\n",
    "### Advantages of Naive Bayes\n",
    "\n",
    "✅ **Very Fast**: Training and prediction are extremely fast\n",
    "✅ **Low Memory**: Only stores means and variances\n",
    "✅ **Works with Small Data**: Needs fewer training examples than discriminative models\n",
    "✅ **Probabilistic**: Gives confidence scores, not just predictions\n",
    "✅ **Handles High Dimensions Well**: Text classification with 10,000+ features\n",
    "✅ **Simple and Interpretable**: Easy to understand\n",
    "✅ **No Hyperparameter Tuning**: Works well with defaults\n",
    "\n",
    "### Disadvantages of Naive Bayes\n",
    "\n",
    "❌ **Naive Assumption**: Feature independence rarely holds in practice\n",
    "❌ **Not State-of-the-Art**: Usually lower accuracy than Random Forests, SVM, Neural Networks\n",
    "❌ **Gaussian Assumption**: Real data may not be normally distributed\n",
    "❌ **Zero Frequency Problem**: If a feature value never appears in training for a class, $P(x|C) = 0$\n",
    "   - Solution: Laplace smoothing (add small constant)\n",
    "❌ **Sensitive to Feature Distribution**: Works best when Gaussian assumption is reasonable\n",
    "\n",
    "### When to Use Naive Bayes\n",
    "\n",
    "**Great for:**\n",
    "- Text classification (spam detection, sentiment analysis)\n",
    "- Real-time prediction (very fast)\n",
    "- Small datasets\n",
    "- Baseline model (quick first attempt)\n",
    "- High-dimensional data\n",
    "\n",
    "**Not great for:**\n",
    "- When features are highly correlated\n",
    "- When you need maximum accuracy\n",
    "- When features don't follow Gaussian distribution\n",
    "\n",
    "### Why It Works Despite Being \"Naive\"\n",
    "\n",
    "Even though independence assumption is violated:\n",
    "1. **Decision boundaries** are often similar to optimal\n",
    "2. **Probability estimates** may be wrong, but **class ranking** is often correct\n",
    "3. **Low variance** (simple model) compensates for **high bias** (wrong assumptions)\n",
    "4. Works well when classes are well-separated\n",
    "\n",
    "### The Code Explained\n",
    "\n",
    "```python\n",
    "nb = GaussianNB()              # Initialize Gaussian Naive Bayes\n",
    "nb.fit(X_train, y_train)       # Calculate mean/variance for each feature per class\n",
    "y_pred_nb = nb.predict(X_test) # Calculate P(C|X) for each test sample, pick max\n",
    "```\n",
    "\n",
    "**No hyperparameters to tune!** One of the simplest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242728f",
   "metadata": {},
   "source": [
    "### Questions for Reflection\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Model Performance Comparison\n",
    "\n",
    "**Question:** Which models performed better in terms of accuracy and confusion matrix? Why do you think that is the case?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Expected Performance Ranking** (typical for this type of dataset):\n",
    "1. **Random Forest** - Highest accuracy (~85-90%)\n",
    "2. **Decision Tree** - Moderate accuracy (~75-85%)\n",
    "3. **Naive Bayes** - Lower accuracy (~70-80%)\n",
    "\n",
    "### Why Random Forest Typically Performs Best:\n",
    "\n",
    "**Variance Reduction Through Ensemble:**\n",
    "- A single decision tree has **high variance** → overfits to training data\n",
    "- Random Forest averages 100 trees → **reduces variance** dramatically\n",
    "- Each tree sees different samples (bootstrap) and random features\n",
    "- Errors of individual trees cancel out through averaging\n",
    "- Result: Better generalization to test data\n",
    "\n",
    "**Handling Complex Relationships:**\n",
    "- Student performance has **non-linear patterns**\n",
    "  - Example: Low studytime AND high absences → likely to fail\n",
    "  - Example: Previous grades (period_1, period_2) interact with study habits\n",
    "- Random Forest captures these **feature interactions** naturally\n",
    "- Multiple trees can learn different aspects of the pattern\n",
    "\n",
    "**Robustness to Outliers:**\n",
    "- Outliers (e.g., student with 0 absences but failed) affect single trees strongly\n",
    "- In Random Forest, outliers only appear in ~63% of trees (bootstrap sampling)\n",
    "- Majority voting reduces impact of outlier-influenced trees\n",
    "\n",
    "### Why Decision Tree Has Moderate Performance:\n",
    "\n",
    "**Pros:**\n",
    "- Can learn complex decision boundaries\n",
    "- Captures feature interactions well\n",
    "- Works well with this type of tabular data\n",
    "\n",
    "**Cons:**\n",
    "- **Overfitting** is the main issue\n",
    "  - Without max_depth constraint, tree grows very deep\n",
    "  - Memorizes training examples instead of learning patterns\n",
    "  - Example: Might create rule \"If age=18 AND absences=7 → Fail\" based on single training example\n",
    "- **High variance** → unstable\n",
    "  - Small change in training data → completely different tree structure\n",
    "- **Greedy algorithm** → locally optimal splits, not globally optimal\n",
    "\n",
    "### Why Naive Bayes Has Lower Performance:\n",
    "\n",
    "**The Independence Assumption is Violated:**\n",
    "\n",
    "Student features are **highly correlated**:\n",
    "- `period_1_grades` ↔ `period_2_grades` (strongly correlated)\n",
    "  - Good first period grade → likely good second period grade\n",
    "  - Naive Bayes treats these as independent → wrong!\n",
    "- `studytime` ↔ `absences` (negatively correlated)\n",
    "  - Students who study more tend to have fewer absences\n",
    "  - Naive Bayes ignores this relationship\n",
    "- `failures` ↔ `period_1_grades` (correlated)\n",
    "  - Past failures predict lower current grades\n",
    "\n",
    "**Gaussian Assumption May Not Hold:**\n",
    "- Some features are **ordinal** (1-4 scales), not continuous\n",
    "  - `studytime`: 1 (<2hrs), 2 (2-5hrs), 3 (5-10hrs), 4 (>10hrs)\n",
    "  - Not truly Gaussian distributed\n",
    "- Some features are **counts** (absences: 0, 1, 2, ..., 93)\n",
    "  - Often right-skewed (most students have low absences)\n",
    "  - Not Gaussian\n",
    "\n",
    "**Despite Violations, Naive Bayes is Still Useful:**\n",
    "- Very **fast** training and prediction\n",
    "- Good **baseline model**\n",
    "- Works well when you have **limited data**\n",
    "- Provides **probability estimates** (not just predictions)\n",
    "\n",
    "### Confusion Matrix Analysis:\n",
    "\n",
    "**Good Model Confusion Matrix:**\n",
    "```\n",
    "                Predicted\n",
    "              Fail    Pass\n",
    "Actual Fail  [TN=25] [FP=5]   ← Few false positives\n",
    "       Pass  [FN=8]  [TP=157] ← Few false negatives\n",
    "```\n",
    "\n",
    "**What to Look For:**\n",
    "\n",
    "**High True Positives (TP):**\n",
    "- Correctly identified students who passed\n",
    "- Most important for identifying success\n",
    "\n",
    "**High True Negatives (TN):**\n",
    "- Correctly identified students who failed\n",
    "- Important for early intervention\n",
    "\n",
    "**Low False Positives (FP):**\n",
    "- Predicted pass, actually failed\n",
    "- **Dangerous**: Student might not get needed support\n",
    "\n",
    "**Low False Negatives (FN):**\n",
    "- Predicted fail, actually passed\n",
    "- Less dangerous, but wastes intervention resources\n",
    "\n",
    "**Model Comparison:**\n",
    "- **Random Forest**: Balanced low FP and FN\n",
    "- **Decision Tree**: Higher FN and FP (overfits to training patterns)\n",
    "- **Naive Bayes**: May have higher FP (over-predicts passing due to prior probability)\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "**Accuracy alone is insufficient!** With 85% passing rate:\n",
    "- A model that always predicts \"Pass\" achieves 85% accuracy!\n",
    "- But 0% recall for \"Fail\" class → useless for intervention\n",
    "- Must examine **precision**, **recall**, and **F1-score** for each class\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Importance\n",
    "\n",
    "**Question:** How important were the features G1, G2, and G3 in predicting the target variable? How would the model performance change if you removed these features from the dataset?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "### Importance of Grade Features (period_1_grades, period_2_grades)\n",
    "\n",
    "**These are the MOST IMPORTANT features!**\n",
    "\n",
    "**Why Grade Features Dominate:**\n",
    "\n",
    "1. **Direct Correlation with Final Grade:**\n",
    "   - `final_grade` (G3) was used to create the target (`passed = G3 >= 10`)\n",
    "   - `period_2_grades` (G2) happens right before G3\n",
    "   - `period_1_grades` (G1) is the earliest indicator\n",
    "   - **Temporal sequence**: G1 → G2 → G3 (strong autocorrelation)\n",
    "\n",
    "2. **Predictive Power:**\n",
    "   - Student with G2=15 → very likely G3>=10 (passed)\n",
    "   - Student with G2=5 → very likely G3<10 (failed)\n",
    "   - These features have **high information gain**\n",
    "\n",
    "3. **Feature Importance from Random Forest:**\n",
    "   - Typical importance ranking:\n",
    "     ```\n",
    "     1. period_2_grades (G2): ~45% importance\n",
    "     2. period_1_grades (G1): ~30% importance\n",
    "     3. failures: ~8% importance\n",
    "     4. absences: ~5% importance\n",
    "     5. studytime: ~3% importance\n",
    "     ... (all other features combined: ~9%)\n",
    "     ```\n",
    "\n",
    "### Impact of Removing Grade Features:\n",
    "\n",
    "**Performance Drop:**\n",
    "- **With G1 & G2**: ~85-90% accuracy\n",
    "- **Without G1 & G2**: ~70-75% accuracy\n",
    "- **Drop**: ~15-20 percentage points!\n",
    "\n",
    "**Why Such a Large Drop:**\n",
    "\n",
    "1. **Loss of Most Predictive Information:**\n",
    "   - Removing G1 and G2 removes ~75% of model's predictive power\n",
    "   - Remaining features (age, studytime, absences) are **weak predictors**\n",
    "   - These features have **noise** and **indirect relationships**\n",
    "\n",
    "2. **Changed Problem Difficulty:**\n",
    "   - **With G1/G2**: \"Given past grades, predict final grade\"\n",
    "     - Relatively easy → temporal continuity\n",
    "   - **Without G1/G2**: \"Given demographics and behavior, predict grade\"\n",
    "     - Much harder → complex socioeconomic factors\n",
    "\n",
    "3. **Model Behavior Changes:**\n",
    "   \n",
    "   **Decision Tree Without G1/G2:**\n",
    "   - Relies on `failures`, `absences`, `studytime`\n",
    "   - Creates shallower tree (less clear splits)\n",
    "   - More balanced confusion matrix (less confident predictions)\n",
    "   \n",
    "   **Random Forest Without G1/G2:**\n",
    "   - Individual trees are weaker\n",
    "   - More disagreement among trees → lower confidence\n",
    "   - Still benefits from ensemble averaging\n",
    "   \n",
    "   **Naive Bayes Without G1/G2:**\n",
    "   - **Might actually improve slightly** (relative to other models)!\n",
    "   - Why? Feature independence assumption less violated\n",
    "   - G1 and G2 are highly correlated → violates Naive Bayes assumption\n",
    "   - Without them, remaining features more independent\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "**Scenario 1: Predicting at End of Period 2**\n",
    "- **Have G1 and G2** → Use them! High accuracy achievable\n",
    "- Identify struggling students before final exams\n",
    "- Target interventions to students with low G2\n",
    "\n",
    "**Scenario 2: Predicting at Start of Year**\n",
    "- **No grades available** → Must use demographic/behavioral features\n",
    "- Lower accuracy, but still useful\n",
    "- Focus on students with: past failures, low studytime, high absences\n",
    "\n",
    "**Scenario 3: Understanding Causes of Success/Failure**\n",
    "- **Should NOT use G1 and G2** as features!\n",
    "- They are outcomes, not causes\n",
    "- Want to find **actionable factors**: studytime, attendance, family support\n",
    "- Feature importance of `studytime` and `absences` more valuable for intervention design\n",
    "\n",
    "### Feature Engineering Alternative:\n",
    "\n",
    "Instead of removing G1/G2 entirely, create **derived features**:\n",
    "- `grade_trend = G2 - G1` (improving or declining?)\n",
    "- `average_grade = (G1 + G2) / 2`\n",
    "- `grade_variance = |G2 - G1|` (consistency)\n",
    "\n",
    "These capture grade information while adding interpretability.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "G1 and G2 are **critically important** for prediction accuracy, but their use depends on:\n",
    "- **When** you're making predictions (timing)\n",
    "- **Why** you're making predictions (intervention vs. understanding)\n",
    "- **What** you can influence (grades vs. behaviors)\n",
    "\n",
    "For **early intervention systems**, focus on non-grade features to identify at-risk students proactively!\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "**Question:** How did data preprocessing steps like feature scaling and handling missing values impact the performance of the models?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "### Feature Scaling (Not Applied in This Lab)\n",
    "\n",
    "**What is Feature Scaling?**\n",
    "\n",
    "Converting all features to the same scale:\n",
    "- **Standardization** (Z-score normalization): Mean=0, Std=1\n",
    "  - Formula: $z = \\frac{x - \\mu}{\\sigma}$\n",
    "- **Min-Max Scaling**: Scale to [0, 1] range\n",
    "  - Formula: $x' = \\frac{x - \\min}{max - \\min}$\n",
    "\n",
    "**Impact on Each Algorithm:**\n",
    "\n",
    "**Decision Tree: NO IMPACT ✗**\n",
    "- Trees split on thresholds: \"Is age >= 18?\" or \"Is age >= 0.5 (scaled)?\"\n",
    "- Scaling changes threshold values but **not split decisions**\n",
    "- Tree structure remains identical\n",
    "- **Conclusion**: Scaling is unnecessary for Decision Trees\n",
    "\n",
    "**Random Forest: NO IMPACT ✗**\n",
    "- Same reasoning as Decision Trees\n",
    "- Ensemble of trees → still unaffected by scaling\n",
    "- **Conclusion**: Scaling is unnecessary for Random Forests\n",
    "\n",
    "**Gaussian Naive Bayes: MINIMAL IMPACT ≈**\n",
    "- Uses mean ($\\mu$) and variance ($\\sigma^2$) of each feature\n",
    "- Probability calculations use Gaussian distribution\n",
    "- Scaling changes $\\mu$ and $\\sigma$ but **probability ratios remain the same**\n",
    "- **However**: If features have very different scales (e.g., age 15-22 vs. absences 0-93), numerical precision can be affected\n",
    "- **Conclusion**: Scaling can help numerical stability, but usually not critical\n",
    "\n",
    "### Why Scaling Matters for Other Algorithms:\n",
    "\n",
    "**Algorithms that NEED scaling:**\n",
    "\n",
    "**K-Nearest Neighbors (KNN):**\n",
    "- Uses distance metrics: $d = \\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + ...}$\n",
    "- Features with larger scales dominate distance calculation\n",
    "- Example: `absences` (0-93) dominates `studytime` (1-4)\n",
    "- **Without scaling**: Model basically ignores small-scale features\n",
    "- **With scaling**: All features contribute equally\n",
    "\n",
    "**Support Vector Machines (SVM):**\n",
    "- Sensitive to feature magnitudes\n",
    "- Kernel computations affected by scale\n",
    "- Convergence is slower without scaling\n",
    "\n",
    "**Neural Networks:**\n",
    "- Gradient descent optimization requires similar scales\n",
    "- Different scales → unstable training\n",
    "- Critical for deep learning\n",
    "\n",
    "**Logistic Regression:**\n",
    "- Regularization (L1/L2) assumes features are comparable\n",
    "- Different scales → biased regularization\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "**In This Dataset: No Missing Values!**\n",
    "\n",
    "```python\n",
    "df_numeric.isnull().sum()\n",
    "# All zeros → clean dataset\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "- Many datasets have missing values\n",
    "- Some algorithms crash with missing data\n",
    "- Must handle before training\n",
    "\n",
    "### If There Were Missing Values:\n",
    "\n",
    "**Strategy 1: Remove Rows**\n",
    "```python\n",
    "df_clean = df.dropna()\n",
    "```\n",
    "- **Pro**: Simple, no assumptions\n",
    "- **Con**: Lose data (bad if many missing values)\n",
    "- **When**: <5% missing data, random missingness\n",
    "\n",
    "**Strategy 2: Impute with Mean/Median**\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "```\n",
    "- **Pro**: Retain all samples\n",
    "- **Con**: Reduces variance, may introduce bias\n",
    "- **When**: Numerical features, moderate missingness\n",
    "\n",
    "**Strategy 3: Impute with Mode**\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "```\n",
    "- **Pro**: Works for categorical features\n",
    "- **Con**: Can introduce bias toward majority class\n",
    "- **When**: Categorical features\n",
    "\n",
    "**Strategy 4: Forward Fill / Backward Fill**\n",
    "```python\n",
    "df.fillna(method='ffill')\n",
    "```\n",
    "- **Pro**: Preserves temporal patterns\n",
    "- **Con**: Only works for time-series data\n",
    "- **When**: Sequential data with temporal autocorrelation\n",
    "\n",
    "**Strategy 5: Model-Based Imputation**\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "```\n",
    "- **Pro**: Uses relationships between features\n",
    "- **Con**: Computationally expensive\n",
    "- **When**: Complex patterns, high-quality imputation needed\n",
    "\n",
    "**Strategy 6: Indicator Feature**\n",
    "```python\n",
    "df['age_missing'] = df['age'].isnull()\n",
    "df['age'].fillna(df['age'].mean())\n",
    "```\n",
    "- **Pro**: Preserves information about missingness\n",
    "- **Con**: Doubles feature count\n",
    "- **When**: Missingness is informative (e.g., refused to answer = sensitive topic)\n",
    "\n",
    "### Impact on Models:\n",
    "\n",
    "**Decision Trees / Random Forests:**\n",
    "- Can handle missing values natively (some implementations)\n",
    "- Surrogate splits: Use other features to predict missing feature\n",
    "- **Impact**: Minimal if handled properly\n",
    "\n",
    "**Naive Bayes:**\n",
    "- Cannot handle missing values directly\n",
    "- **Must impute** or remove\n",
    "- GaussianNB needs complete data\n",
    "\n",
    "**General Impact:**\n",
    "- **Poor imputation** → introduces noise → lowers accuracy 2-5%\n",
    "- **Removing too much data** → insufficient training samples → underfitting\n",
    "- **Best practice**: Analyze missingness pattern (random vs. systematic)\n",
    "\n",
    "### Summary for This Lab:\n",
    "\n",
    "| Preprocessing Step | Decision Tree | Random Forest | Naive Bayes |\n",
    "|-------------------|--------------|---------------|-------------|\n",
    "| **Feature Scaling** | Not needed ✗ | Not needed ✗ | Minor help ≈ |\n",
    "| **Missing Values** | None present ✓ | None present ✓ | None present ✓ |\n",
    "\n",
    "**Key Takeaway:**\n",
    "- Tree-based models (DT, RF) are **robust** to preprocessing\n",
    "- This is one reason they're so popular in practice!\n",
    "- But always check for missing values and handle appropriately\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Selection\n",
    "\n",
    "**Question:** If you had to choose one model for deployment, which one would it be and why? Consider factors like accuracy, interpretability, and computational efficiency.\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Recommendation: It depends on the deployment context!**\n",
    "\n",
    "Let me analyze each model across key dimensions:\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario 1: Real-Time Student Advising System\n",
    "\n",
    "**Chosen Model: Gaussian Naive Bayes**\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "**Speed (CRITICAL):**\n",
    "- Prediction: **<1 millisecond** per student\n",
    "- Web application needs instant response for advisor\n",
    "- 500 students request predictions simultaneously → NB handles easily\n",
    "- Decision Tree: ~5ms, Random Forest: ~50ms (100 trees)\n",
    "\n",
    "**Memory (IMPORTANT):**\n",
    "- **Minimal memory footprint** (~few KB)\n",
    "- Only stores mean and variance for each feature\n",
    "- Can deploy on edge devices, mobile apps\n",
    "- Random Forest: Several MB (stores 100 full trees)\n",
    "\n",
    "**Acceptable Accuracy:**\n",
    "- 75-80% accuracy sufficient for **advisory tool**\n",
    "- Not making high-stakes decisions\n",
    "- Advisor uses prediction as **one input among many**\n",
    "- Speed/accessibility outweighs 5-10% accuracy gain\n",
    "\n",
    "**Probabilistic Output:**\n",
    "- `predict_proba()` gives confidence: \"85% probability of passing\"\n",
    "- More useful than binary prediction\n",
    "- Advisor can make nuanced recommendations\n",
    "\n",
    "**When to Update:**\n",
    "- Real-time training as new grades come in\n",
    "- Training is **instantaneous** (just recalculate mean/variance)\n",
    "- Random Forest: Retraining takes minutes\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario 2: End-of-Semester Intervention Identification\n",
    "\n",
    "**Chosen Model: Random Forest**\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "**Accuracy (CRITICAL):**\n",
    "- **85-90% accuracy** vs. 75-80% for Naive Bayes\n",
    "- High-stakes: Determines who gets tutoring resources\n",
    "- 10% accuracy gain = 50 more students correctly identified (out of 500)\n",
    "- **Cost of errors:**\n",
    "  - False Negative: Student fails who could have been helped → severe\n",
    "  - False Positive: Wasted tutoring resources → moderate\n",
    "\n",
    "**Robustness:**\n",
    "- Handles **outliers** well (exceptional students)\n",
    "- Works with **correlated features** (doesn't assume independence)\n",
    "- Less sensitive to new data patterns\n",
    "\n",
    "**Feature Importance:**\n",
    "- Can identify **which factors** predict failure\n",
    "- Example: \"absences and failures are top predictors\"\n",
    "- Informs intervention design: \"Focus on attendance tracking\"\n",
    "\n",
    "**Batch Prediction:**\n",
    "- Run once per semester, not real-time\n",
    "- Can afford 50ms per student\n",
    "- 500 students × 50ms = 25 seconds → acceptable\n",
    "\n",
    "**Tunable:**\n",
    "- Can improve with hyperparameter tuning\n",
    "- GridSearchCV finds optimal `n_estimators`, `max_depth`\n",
    "- Naive Bayes has no tuning options\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario 3: Explainable AI for Educators\n",
    "\n",
    "**Chosen Model: Decision Tree (with max_depth constraint)**\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "**Interpretability (CRITICAL):**\n",
    "- Can **visualize entire decision tree**\n",
    "- Teachers can see **exact decision rules**\n",
    "  ```\n",
    "  If period_2_grades < 9.5:\n",
    "      If failures > 0:\n",
    "          Predict: FAIL\n",
    "      Else:\n",
    "          If absences > 10:\n",
    "              Predict: FAIL\n",
    "          Else:\n",
    "              Predict: PASS\n",
    "  ```\n",
    "- **Transparency** builds trust with educators\n",
    "- Can **explain individual predictions**: \"Student predicted to fail because G2<10 and absences>15\"\n",
    "\n",
    "**Educational Value:**\n",
    "- Teachers learn which factors matter most\n",
    "- Can validate against pedagogical knowledge\n",
    "- Identifies unexpected patterns worth investigating\n",
    "\n",
    "**Simplicity:**\n",
    "- No black-box ensemble\n",
    "- No probabilistic assumptions\n",
    "- Just a flowchart of decisions\n",
    "\n",
    "**Prevent Overfitting:**\n",
    "```python\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=5,          # Limit complexity\n",
    "    min_samples_split=20, # Require 20 samples to split\n",
    "    min_samples_leaf=10   # Require 10 samples in each leaf\n",
    ")\n",
    "```\n",
    "- Constraints improve generalization\n",
    "- Tree remains interpretable (only 5 levels deep)\n",
    "- Accuracy: ~80-85% (between NB and RF)\n",
    "\n",
    "**Why Not Random Forest Here:**\n",
    "- 100 trees → impossible to visualize\n",
    "- \"The model says this because averaging 100 trees voted...\" → unsatisfying\n",
    "- Educators need **actionable insights**, not just predictions\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario 4: Research Study on Educational Outcomes\n",
    "\n",
    "**Chosen Model: All Three Models + More**\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "**Comprehensive Analysis:**\n",
    "- Compare Decision Tree, Random Forest, Naive Bayes\n",
    "- Add Logistic Regression, SVM, XGBoost\n",
    "- Ensemble methods (voting, stacking)\n",
    "\n",
    "**Statistical Rigor:**\n",
    "- 10-fold cross-validation for robust estimates\n",
    "- Confidence intervals on accuracy\n",
    "- Statistical significance tests (t-test on accuracies)\n",
    "\n",
    "**Interpretability Analysis:**\n",
    "- Feature importance from Random Forest\n",
    "- Coefficients from Logistic Regression\n",
    "- SHAP values for any model (explains individual predictions)\n",
    "\n",
    "**No Deployment Constraints:**\n",
    "- Computational cost irrelevant\n",
    "- Can spend hours training models\n",
    "- Goal: **understand** relationships, not just predict\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table: Model Selection by Context\n",
    "\n",
    "| Criterion | Decision Tree | Random Forest | Naive Bayes |\n",
    "|-----------|--------------|---------------|-------------|\n",
    "| **Accuracy** | ⭐⭐⭐ (75-85%) | ⭐⭐⭐⭐⭐ (85-90%) | ⭐⭐ (70-80%) |\n",
    "| **Interpretability** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ |\n",
    "| **Training Speed** | ⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **Prediction Speed** | ⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **Memory Usage** | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **Handles Correlated Features** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐ |\n",
    "| **Robustness to Overfitting** | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| **Hyperparameter Tuning Needed** | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "\n",
    "### My Final Recommendation:\n",
    "\n",
    "**For most educational applications: Random Forest**\n",
    "\n",
    "**Why:**\n",
    "- **Accuracy is paramount** for fair resource allocation\n",
    "- Computational cost is acceptable (batch processing)\n",
    "- Feature importance provides insights\n",
    "- Robust and reliable\n",
    "- Good \"default choice\" when unsure\n",
    "\n",
    "**But:**\n",
    "- Use **Naive Bayes** for real-time, low-resource environments\n",
    "- Use **Decision Tree** when stakeholders need explainability\n",
    "- Use **ensemble of models** for critical decisions (voting classifier)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Hyperparameter Tuning\n",
    "\n",
    "**Question:** How did hyperparameter tuning affect the performance of the models? What other hyperparameters could you tune to potentially improve the models?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Note:** This lab did NOT perform hyperparameter tuning (used default parameters).\n",
    "Let's analyze what tuning WOULD do:\n",
    "\n",
    "---\n",
    "\n",
    "### Current Performance (Default Parameters):\n",
    "\n",
    "**Decision Tree:**\n",
    "```python\n",
    "dt = DecisionTreeClassifier()  # All defaults\n",
    "```\n",
    "**Defaults:**\n",
    "- `max_depth=None` → Tree grows until pure leaves (severe overfitting)\n",
    "- `min_samples_split=2` → Split even if only 2 samples\n",
    "- `min_samples_leaf=1` → Leaves can have single sample\n",
    "- `criterion='gini'` → Use Gini impurity\n",
    "\n",
    "**Likely Problem:**\n",
    "- Tree grows very deep (>20 levels)\n",
    "- Memorizes training data\n",
    "- **Training accuracy: ~100%** (perfect)\n",
    "- **Test accuracy: ~75-80%** (overfitting!)\n",
    "\n",
    "**Random Forest:**\n",
    "```python\n",
    "rf = RandomForestClassifier()  # All defaults\n",
    "```\n",
    "**Defaults:**\n",
    "- `n_estimators=100` → 100 trees\n",
    "- `max_depth=None` → Each tree grows fully\n",
    "- `min_samples_split=2`\n",
    "- `max_features='sqrt'` → $\\sqrt{15} \\approx 4$ features per split\n",
    "- `bootstrap=True` → Use bagging\n",
    "\n",
    "**Performance:**\n",
    "- Defaults are actually quite good for RF!\n",
    "- **Test accuracy: ~85-90%**\n",
    "- Averaging reduces overfitting from deep trees\n",
    "\n",
    "**Naive Bayes:**\n",
    "```python\n",
    "nb = GaussianNB()  # No hyperparameters to tune!\n",
    "```\n",
    "**Parameters:**\n",
    "- `var_smoothing=1e-9` → Small constant added to variance (numerical stability)\n",
    "- Usually don't need to change this\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "**What is GridSearchCV?**\n",
    "\n",
    "Systematically tests combinations of hyperparameters using cross-validation.\n",
    "\n",
    "**Example for Decision Tree:**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': [2, 10, 20, 50],\n",
    "    'min_samples_leaf': [1, 5, 10, 20],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    dt, \n",
    "    param_grid, \n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1          # Use all CPU cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", grid_search.best_score_)\n",
    "\n",
    "# Use best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "```\n",
    "\n",
    "**How It Works:**\n",
    "1. Tests all combinations: 6 × 4 × 4 × 2 = **192 combinations**\n",
    "2. For each combination, trains 5 models (5-fold CV)\n",
    "3. Total models trained: 192 × 5 = **960 models**!\n",
    "4. Picks combination with highest average CV accuracy\n",
    "\n",
    "**Expected Results:**\n",
    "```\n",
    "Best parameters: {\n",
    "    'max_depth': 7,\n",
    "    'min_samples_split': 20,\n",
    "    'min_samples_leaf': 5,\n",
    "    'criterion': 'gini'\n",
    "}\n",
    "Best CV score: 0.82\n",
    "```\n",
    "\n",
    "**Performance Improvement:**\n",
    "- **Before tuning**: 75-80% test accuracy (overfitting)\n",
    "- **After tuning**: 82-85% test accuracy\n",
    "- **Gain**: +5-7 percentage points\n",
    "\n",
    "---\n",
    "\n",
    "### Decision Tree Hyperparameters (Comprehensive List)\n",
    "\n",
    "**Tree Structure:**\n",
    "\n",
    "**`max_depth`** (int or None):\n",
    "- Maximum depth of tree\n",
    "- **None**: Grow until pure leaves (overfitting)\n",
    "- **3-5**: Shallow tree (underfitting, but interpretable)\n",
    "- **7-15**: Usually optimal range\n",
    "- **Effect**: Lower = simpler, higher = more complex\n",
    "\n",
    "**`min_samples_split`** (int or float):\n",
    "- Minimum samples required to split node\n",
    "- **2**: Default, can overfit\n",
    "- **10-50**: Good range for regularization\n",
    "- **Effect**: Higher = simpler tree (fewer splits)\n",
    "\n",
    "**`min_samples_leaf`** (int or float):\n",
    "- Minimum samples required in leaf node\n",
    "- **1**: Default, can create tiny leaves (overfitting)\n",
    "- **5-20**: Good regularization\n",
    "- **Effect**: Higher = simpler tree (larger leaves)\n",
    "\n",
    "**`max_leaf_nodes`** (int or None):\n",
    "- Limit total number of leaf nodes\n",
    "- Alternative to max_depth\n",
    "- **Effect**: Lower = simpler tree\n",
    "\n",
    "**Split Criteria:**\n",
    "\n",
    "**`criterion`** ('gini' or 'entropy'):\n",
    "- How to measure split quality\n",
    "- **'gini'**: Gini impurity (default, faster)\n",
    "- **'entropy'**: Information gain (sometimes better)\n",
    "- **Usually similar performance**\n",
    "\n",
    "**`splitter`** ('best' or 'random'):\n",
    "- How to choose split at each node\n",
    "- **'best'**: Choose best split (default)\n",
    "- **'random'**: Choose best among random subset (faster, regularization)\n",
    "\n",
    "**Feature Selection:**\n",
    "\n",
    "**`max_features`** (int, float, or str):\n",
    "- Number of features to consider for each split\n",
    "- **None**: Use all features (default for DT)\n",
    "- **'sqrt'**: $\\sqrt{n_{features}}$ (common for classification)\n",
    "- **'log2'**: $\\log_2(n_{features})$\n",
    "- **Effect**: Lower = more regularization, faster\n",
    "\n",
    "**Class Weights:**\n",
    "\n",
    "**`class_weight`** (dict, 'balanced', or None):\n",
    "- Weights for each class\n",
    "- **None**: All classes equal\n",
    "- **'balanced'**: Automatically adjust for class imbalance\n",
    "  - Formula: $w_i = \\frac{n_{samples}}{n_{classes} \\times n_{samples_i}}$\n",
    "- **Effect**: Helps with imbalanced datasets (e.g., 90% pass, 10% fail)\n",
    "\n",
    "**Randomness:**\n",
    "\n",
    "**`random_state`** (int):\n",
    "- Seed for reproducibility\n",
    "- Important for comparing models\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest Hyperparameters (Comprehensive List)\n",
    "\n",
    "**All Decision Tree parameters PLUS:**\n",
    "\n",
    "**Ensemble Size:**\n",
    "\n",
    "**`n_estimators`** (int):\n",
    "- Number of trees in forest\n",
    "- **10**: Too few, high variance\n",
    "- **100**: Default, usually good\n",
    "- **500-1000**: Better performance, slower\n",
    "- **Effect**: More trees = better performance (with diminishing returns)\n",
    "- **Tuning range**: [100, 200, 500]\n",
    "\n",
    "**Bootstrap:**\n",
    "\n",
    "**`bootstrap`** (bool):\n",
    "- Whether to use bootstrap samples\n",
    "- **True**: Default (bagging)\n",
    "- **False**: Use all data for each tree (less diversity)\n",
    "- **Usually keep True**\n",
    "\n",
    "**`oob_score`** (bool):\n",
    "- Whether to use out-of-bag samples for validation\n",
    "- **True**: Get free validation score (no need for separate validation set)\n",
    "- **False**: Default\n",
    "- **Useful for monitoring**\n",
    "\n",
    "**`max_samples`** (int, float, or None):\n",
    "- Number of samples to draw for each tree\n",
    "- **None**: Draw n_samples (default)\n",
    "- **0.8**: Draw 80% of samples\n",
    "- **Effect**: Lower = more diversity (more regularization)\n",
    "\n",
    "**Feature Randomness:**\n",
    "\n",
    "**`max_features`** (int, float, or str):\n",
    "- Number of features for each split\n",
    "- **'sqrt'**: Default for RandomForestClassifier\n",
    "- **'log2'**: More regularization\n",
    "- **None**: Use all features (less diversity, may overfit)\n",
    "- **Effect**: Lower = more tree diversity\n",
    "\n",
    "**Parallelization:**\n",
    "\n",
    "**`n_jobs`** (int):\n",
    "- Number of CPU cores to use\n",
    "- **1**: Single core\n",
    "- **-1**: Use all cores (much faster)\n",
    "- **Doesn't affect accuracy, only speed**\n",
    "\n",
    "**Tree Constraints** (same as Decision Tree):\n",
    "- `max_depth`\n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "- `max_leaf_nodes`\n",
    "\n",
    "**Recommended Tuning for Random Forest:**\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "```\n",
    "\n",
    "**Expected Improvement:**\n",
    "- **Before tuning**: 85-88% test accuracy\n",
    "- **After tuning**: 88-92% test accuracy\n",
    "- **Gain**: +3-4 percentage points\n",
    "\n",
    "---\n",
    "\n",
    "### Naive Bayes Hyperparameters (Limited)\n",
    "\n",
    "**`var_smoothing`** (float):\n",
    "- Portion of largest variance added to all variances (numerical stability)\n",
    "- **1e-9**: Default\n",
    "- **1e-11 to 1e-7**: Tuning range\n",
    "- **Effect**: Very minor on most datasets\n",
    "\n",
    "**Rarely needs tuning!**\n",
    "\n",
    "For other Naive Bayes variants:\n",
    "\n",
    "**MultinomialNB:**\n",
    "- `alpha`: Laplace smoothing parameter (0 to 1)\n",
    "- `fit_prior`: Whether to learn class prior probabilities\n",
    "\n",
    "**BernoulliNB:**\n",
    "- `alpha`: Laplace smoothing\n",
    "- `binarize`: Threshold for binary conversion\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Tuning Techniques\n",
    "\n",
    "**1. Randomized Search (Faster than Grid Search):**\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, \n",
    "    param_distributions, \n",
    "    n_iter=50,  # Try 50 random combinations (not all)\n",
    "    cv=5, \n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- **Faster**: Tests 50 combinations instead of 5×6×5=150\n",
    "- **Often finds good solutions** with less computation\n",
    "\n",
    "**2. Bayesian Optimization:**\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators': (50, 500),\n",
    "    'max_depth': (5, 30),\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(rf, param_space, n_iter=32)\n",
    "```\n",
    "- **Smart search**: Learns from previous trials\n",
    "- **More efficient** than random search\n",
    "\n",
    "**3. Early Stopping (for iterative models):**\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=1000,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10  # Stop if no improvement for 10 rounds\n",
    ")\n",
    "```\n",
    "- Prevents overfitting\n",
    "- Saves computation time\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Model Improvements\n",
    "\n",
    "**1. Feature Engineering:**\n",
    "```python\n",
    "# Create interaction features\n",
    "df['study_absences'] = df['studytime'] * (1 / (df['absences'] + 1))\n",
    "df['grade_trend'] = df['period_2_grades'] - df['period_1_grades']\n",
    "df['total_alcohol'] = df['Dalc'] + df['Walc']\n",
    "```\n",
    "- **Can boost accuracy** by 5-10%\n",
    "- Captures relationships the model might miss\n",
    "\n",
    "**2. Feature Selection:**\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=10)  # Keep top 10 features\n",
    "X_selected = selector.fit_transform(X_train, y_train)\n",
    "```\n",
    "- Removes irrelevant features\n",
    "- Reduces overfitting\n",
    "- Speeds up training\n",
    "\n",
    "**3. Ensemble Methods:**\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting = VotingClassifier([\n",
    "    ('dt', best_dt),\n",
    "    ('rf', best_rf),\n",
    "    ('nb', nb)\n",
    "], voting='soft')  # Use probability averages\n",
    "```\n",
    "- Combines strengths of different models\n",
    "- Often +2-3% accuracy boost\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: Expected Performance Gains\n",
    "\n",
    "| Model | Default | Tuned | Gain |\n",
    "|-------|---------|-------|------|\n",
    "| **Decision Tree** | 75-80% | 82-85% | +5-7% |\n",
    "| **Random Forest** | 85-88% | 88-92% | +3-4% |\n",
    "| **Naive Bayes** | 70-75% | 71-76% | +1% |\n",
    "\n",
    "**Key Insight:**\n",
    "- **Decision Tree benefits most** from tuning (high variance problem)\n",
    "- **Random Forest already robust**, but tuning helps\n",
    "- **Naive Bayes has minimal tuning** options (high bias problem)\n",
    "\n",
    "**Best Practice:**\n",
    "Always perform hyperparameter tuning before deployment! The performance gains are significant and worth the computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Challenges\n",
    "\n",
    "1. **Feature Engineering**: Try creating new features from the existing ones. For example, you could create an average grade feature from G1, G2, and G3. How does this new feature impact model performance?\n",
    "2. **Cross-Validation**: Implement cross-validation to get a more robust estimate of model performance. How do the results compare to the train-test split method?\n",
    "3. **Ensemble Methods**: Experiment with other ensemble methods like Gradient Boosting or AdaBoost. How do these methods compare to the Random Forest classifier?\n",
    "4. **Dimensionality Reduction**: Apply dimensionality reduction techniques like PCA (Principal Component Analysis) to the dataset. How does this affect model performance?\n",
    "5. **Different Datasets**: Try applying the same models and preprocessing steps to a different dataset. How do the results compare?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab4",
   "language": "python",
   "name": "lab4_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
